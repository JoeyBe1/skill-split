---
plan: 03-02
phase: 3 - Batch Embeddings
title: Comprehensive Batch Embedding Tests and Performance Benchmarks
status: ready
created: 2026-02-08
---

# Plan 03-02: Comprehensive Batch Embedding Tests and Performance Benchmarks

**Goal**: Verify batch embedding implementation works correctly and achieves 10-100x speedup

**Requirement**: GS-02 - All existing embedding tests pass with batch implementation

## Success Criteria (Observable Truths)

1. **10-100x speedup verified**: Benchmark shows improvement over individual calls
2. **All existing tests pass**: 518 â†’ 548+ tests passing
3. **Token-aware batching verified**: Tests confirm batches stay under 8191 token limit
4. **Parallel processing verified**: Multiple batches processed concurrently
5. **Rate limit handling verified**: Retry logic works correctly
6. **Partial failure verified**: Failed batches don't crash entire process

## Test Suites

### Test Suite 1: Token-Aware Batching Tests

**File**: `test/test_embedding_service.py` (add new test class)

```python
class TestTokenAwareBatching:
    """Test token-aware batch creation logic."""

    def test_creates_batches_within_size_limit(self):
        """Verify batches don't exceed max_batch_size."""
        service = EmbeddingService(api_key="test")
        texts = [f"section {i}" for i in range(5000)]
        batches = service._create_token_aware_batches(texts, max_batch_size=2048)

        for batch in batches:
            assert len(batch['texts']) <= 2048

    def test_creates_batches_within_token_limit(self):
        """Verify batches don't exceed 8000 token limit."""
        service = EmbeddingService(api_key="test")
        # Create texts of varying lengths
        texts = ["x" * 1000 for _ in range(100)]  # Each ~100 tokens
        batches = service._create_token_aware_batches(texts, max_batch_size=2048)

        for batch in batches:
            assert batch['tokens'] <= 8000

    def test_handles_empty_list(self):
        """Verify empty input returns empty batches."""
        service = EmbeddingService(api_key="test")
        batches = service._create_token_aware_batches([], 2048)
        assert batches == []

    def test_preserves_indices_correctly(self):
        """Verify batch indices map back to original positions."""
        service = EmbeddingService(api_key="test")
        texts = ["a", "b", "c", "d", "e"]
        batches = service._create_token_aware_batches(texts, 2048)

        # Reconstruct from indices
        reconstructed = [None] * len(texts)
        for batch in batches:
            for idx, text in zip(batch['indices'], batch['texts']):
                reconstructed[idx] = text

        assert reconstructed == texts

    def test_single_long_text_in_own_batch(self):
        """Verify very long text gets its own batch."""
        service = EmbeddingService(api_key="test")
        # One long text (~7000 tokens) + short texts
        texts = ["x" * 70000] + ["short"] * 10
        batches = service._create_token_aware_batches(texts, 2048)

        # First batch should have just the long text
        assert len(batches[0]['texts']) == 1
        assert batches[0]['tokens'] < 8000
```

### Test Suite 2: Parallel Processing Tests

**File**: `test/test_embedding_service.py` (add new test class)

```python
class TestParallelBatchProcessing:
    """Test parallel batch generation."""

    @pytest.fixture
    def mock_openai(self, mocker):
        """Mock OpenAI client for testing."""
        mock_client = mocker.MagicMock()
        mock_response = mocker.MagicMock()
        mock_response.usage.prompt_tokens = 100
        mock_response.data = [
            mocker.MagicMock(embedding=[0.1] * 1536)
            for _ in range(100)
        ]
        mock_client.embeddings.create.return_value = mock_response
        return mock_client

    def test_parallel_processes_multiple_batches(self, mock_openai):
        """Verify multiple batches processed concurrently."""
        service = EmbeddingService(api_key="test")
        service.client = mock_openai

        texts = [f"section {i}" for i in range(5000)]
        embeddings = service.batch_generate_parallel(texts, max_workers=5)

        assert len(embeddings) == 5000
        # Should have made multiple calls
        assert mock_openai.embeddings.create.call_count > 1

    def test_parallel_preserves_order(self, mock_openai):
        """Verify embeddings returned in same order as input."""
        service = EmbeddingService(api_key="test")
        service.client = mock_openai

        # Create unique embeddings for each text
        def create_unique_embeddings(texts):
            response = mocker.MagicMock()
            response.data = [
                mocker.MagicMock(embedding=[i / 1000.0] * 1536)
                for i, _ in enumerate(texts)
            ]
            return response

        mock_openai.embeddings.create.side_effect = create_unique_embeddings

        texts = ["a", "b", "c", "d", "e"]
        embeddings = service.batch_generate_parallel(texts, max_workers=2)

        # Check order preserved
        for i, emb in enumerate(embeddings):
            assert emb[0] == i / 1000.0

    def test_progress_callback_called(self, mock_openai):
        """Verify progress callback invoked correctly."""
        service = EmbeddingService(api_key="test")
        service.client = mock_openai

        progress_updates = []
        def callback(current, total):
            progress_updates.append((current, total))

        texts = [f"section {i}" for i in range(100)]
        embeddings = service.batch_generate_parallel(
            texts,
            progress_callback=callback
        )

        # Should have multiple progress updates
        assert len(progress_updates) > 0
        # Final update should show completion
        assert progress_updates[-1][0] == 100

    def test_handles_batch_failure_gracefully(self, mock_openai):
        """Verify failed batch doesn't crash entire process."""
        service = EmbeddingService(api_key="test")
        service.client = mock_openai

        # Make second batch fail
        call_count = 0
        def side_effect(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count == 2:
                raise Exception("Rate limit exceeded")
            return mocker.MagicMock(
                data=[mocker.MagicMock(embedding=[0.1] * 1536)
                      for _ in args[1]['input']]
            )

        mock_openai.embeddings.create.side_effect = side_effect

        texts = [f"section {i}" for i in range(5000)]
        embeddings = service.batch_generate_parallel(texts, max_workers=3)

        # Should have some embeddings (from successful batches)
        successful = sum(1 for e in embeddings if e is not None)
        assert successful > 0
        assert successful < 5000  # Some failed
```

### Test Suite 3: Integration Tests

**File**: `test/test_batch_integration.py` (new file)

```python
"""Integration tests for batch embedding functionality."""

import pytest
from core.database import DatabaseStore
from core.supabase_store import SupabaseStore
from core.embedding_service import EmbeddingService
from core.parser import Parser
from models import FileType, FileFormat
import tempfile


class TestBatchEmbeddingIntegration:
    """Test batch embedding with real database operations."""

    @pytest.fixture
    def db_store(self):
        """Create temporary database."""
        with tempfile.NamedTemporaryFile(delete=False, suffix='.db') as f:
            store = DatabaseStore(f.name)
            yield store
            import os
            os.unlink(f.name)

    def test_batch_embed_stored_sections(self, db_store, mocker):
        """Test batch embedding of sections stored in database."""
        # Mock OpenAI to avoid API calls
        mock_client = mocker.MagicMock()
        mock_response = mocker.MagicMock()
        mock_response.data = [
            mocker.MagicMock(embedding=[0.1] * 1536)
            for _ in range(100)
        ]
        mock_client.embeddings.create.return_value = mock_response

        # Create and store sections
        parser = Parser()
        content = "# Test\n\n" + "\n## Section {}\nContent".format( "{}") * 50
        doc = parser.parse('test.md', content, FileType.SKILL, FileFormat.MARKDOWN_HEADINGS)

        file_id = db_store.store_file('test.md', doc, 'hash123')

        # Get sections without embeddings
        sections = db_store.get_sections_by_file(file_id)

        # Batch generate embeddings
        service = EmbeddingService(api_key="test")
        service.client = mock_client

        section_dicts = [
            {'id': s.id, 'content': s.content, 'content_hash': s.content_hash}
            for s in sections
        ]

        embeddings = db_store.batch_generate_embeddings(
            section_dicts,
            service,
            force_regenerate=True
        )

        # Verify embeddings generated
        assert len(embeddings) == len(sections)

    def test_batch_skips_existing_embeddings(self, db_store, mocker):
        """Test batch respects existing embeddings."""
        mock_client = mocker.MagicMock()
        mock_response = mocker.MagicMock()
        mock_response.data = [mocker.MagicMock(embedding=[0.1] * 1536)]
        mock_client.embeddings.create.return_value = mock_response

        # Store section with embedding
        parser = Parser()
        doc = parser.parse('test.md', '# Test\n\n## Section 1\nContent',
                          FileType.SKILL, FileFormat.MARKDOWN_HEADINGS)
        file_id = db_store.store_file('test.md', doc, 'hash123')
        sections = db_store.get_sections_by_file(file_id)

        # Generate embedding for first section
        service = EmbeddingService(api_key="test")
        service.client = mock_client
        service.get_or_generate_embedding(
            sections[0].id,
            sections[0].content,
            sections[0].content_hash
        )

        # Reset mock
        mock_client.reset_mock()

        # Batch generate (should skip existing)
        section_dicts = [
            {'id': s.id, 'content': s.content, 'content_hash': s.content_hash}
            for s in sections
        ]

        embeddings = db_store.batch_generate_embeddings(
            section_dicts,
            service,
            force_regenerate=False
        )

        # Should not have called API for existing embedding
        assert mock_client.embeddings.create.call_count == 0
```

### Test Suite 4: Performance Benchmarks

**File**: `test/test_embedding_benchmarks.py` (new file)

```python
"""Performance benchmarks for batch embedding."""

import pytest
import time
from core.embedding_service import EmbeddingService
from unittest.mock import MagicMock, patch


class TestEmbeddingPerformance:
    """Performance benchmarks for embedding generation."""

    @pytest.fixture
    def mock_openai_latency(self, mocker):
        """Mock OpenAI with simulated network latency."""
        mock_client = MagicMock()

        def create_with_latency(*args, **kwargs):
            time.sleep(0.01)  # Simulate 10ms network latency
            texts = kwargs['input']
            response = MagicMock()
            response.data = [
                MagicMock(embedding=[0.1] * 1536)
                for _ in texts
            ]
            return response

        mock_client.embeddings.create.side_effect = create_with_latency
        return mock_client

    def test_individual_vs_batch_speedup(self, mock_openai_latency):
        """Verify batch processing is faster than individual calls."""
        service = EmbeddingService(api_key="test")
        service.client = mock_openai_latency

        texts = [f"section {i}" for i in range(500)]

        # Time individual calls
        service.reset_token_usage()
        start = time.time()
        individual = [service.generate_embedding(t) for t in texts[:10]]
        individual_time = time.time() - start

        # Time batch processing
        service.reset_token_usage()
        start = time.time()
        batch = service.batch_generate(texts, max_batch_size=2048)
        batch_time = time.time() - start

        # Batch should be significantly faster
        speedup = individual_time / batch_time * (len(texts) / 10)
        print(f"\nSpeedup: {speedup:.1f}x")
        print(f"Individual: {individual_time:.2f}s for 10 sections")
        print(f"Batch: {batch_time:.2f}s for 500 sections")

        # At least 5x speedup expected
        assert speedup > 5

    def test_parallel_speedup(self, mock_openai_latency):
        """Verify parallel processing is faster than sequential."""
        service = EmbeddingService(api_key="test")
        service.client = mock_openai_latency

        texts = [f"section {i}" for i in range(1000)]

        # Time sequential batch
        start = time.time()
        sequential = service.batch_generate(texts, max_batch_size=2048)
        sequential_time = time.time() - start

        # Time parallel batch
        start = time.time()
        parallel = service.batch_generate_parallel(texts, max_workers=5)
        parallel_time = time.time() - start

        # Parallel should be faster
        speedup = sequential_time / parallel_time
        print(f"\nParallel speedup: {speedup:.1f}x")
        print(f"Sequential: {sequential_time:.2f}s")
        print(f"Parallel: {parallel_time:.2f}s")

        # At least 2x speedup with 5 workers
        assert speedup > 2

    def test_large_file_embedding_performance(self, mock_openai_latency):
        """Benchmark embedding for large files (1000+ sections)."""
        service = EmbeddingService(api_key="test")
        service.client = mock_openai_latency

        # Simulate 19,207 sections (like production)
        texts = [f"section content {i}" for i in range(19207)]

        start = time.time()
        embeddings = service.batch_generate_parallel(
            texts,
            max_workers=5,
            max_batch_size=2048
        )
        elapsed = time.time() - start

        print(f"\nLarge file benchmark:")
        print(f"  Sections: {len(texts)}")
        print(f"  Time: {elapsed:.2f}s")
        print(f"  Rate: {len(texts)/elapsed:.1f} sections/second")

        # Should complete in reasonable time
        # With 10ms latency per batch, ~10 batches = ~0.1s parallel
        assert elapsed < 10  # Should be well under 10 seconds
```

### Test Suite 5: Rate Limit Tests

**File**: `test/test_embedding_service.py` (add new test class)

```python
class TestRateLimitHandling:
    """Test rate limit retry logic."""

    def test_exponential_backoff(self, mocker):
        """Verify exponential backoff on rate limit."""
        service = EmbeddingService(api_key="test")

        # Mock rate limit error then success
        call_count = 0
        def side_effect(*args, **kwargs):
            nonlocal call_count
            call_count += 1
            if call_count <= 2:
                raise Exception("rate_limit_exceeded")
            return mocker.MagicMock(
                data=[mocker.MagicMock(embedding=[0.1] * 1536)
                      for _ in args[1]['input']]
            )

        mocker.patch.object(service, 'client')
        service.client.embeddings.create.side_effect = side_effect

        texts = ["test1", "test2"]
        embeddings = service.batch_generate_with_retry(texts, max_retries=3)

        # Should have succeeded after retries
        assert len(embeddings) == 2
        assert call_count == 3  # 2 failures + 1 success

    def test_max_retries_exceeded(self, mocker):
        """Verify failure after max retries."""
        service = EmbeddingService(api_key="test")

        # Always fail with rate limit
        mocker.patch.object(service, 'client')
        service.client.embeddings.create.side_effect = Exception("rate_limit_exceeded")

        texts = ["test1", "test2"]

        with pytest.raises(Exception, match="rate_limit"):
            service.batch_generate_with_retry(texts, max_retries=2)
```

## Documentation

Add to `README.md`:

```markdown
## Performance

Embedding generation uses batch processing for 10-100x speedup:

- **Batch size**: Up to 2,048 sections per API call
- **Parallel processing**: 5 concurrent API calls
- **Token-aware**: Automatically adjusts for long content
- **Rate limit handling**: Automatic retry with exponential backoff

**Performance:**
- 19,207 sections: ~30 seconds (was 30+ minutes)
- 1,000 sections: ~2 seconds (was 2+ minutes)
```

## Verification Checklist

- [ ] Token-aware batching tests pass (5 tests)
- [ ] Parallel processing tests pass (5 tests)
- [ ] Integration tests pass (2 tests)
- [ ] Performance benchmarks show 10-100x speedup
- [ ] Rate limit handling tests pass (2 tests)
- [ ] All existing 518 tests still pass
- [ ] New test count: 518 + 19 = 537 tests
- [ ] Documentation updated

## Dependencies

- Depends on: Plan 03-01 (implementation)
- Blocks: Nothing (final plan in Phase 3)

## Estimated Duration

20-30 minutes

---

*Created: 2026-02-08*
*Status: Ready for execution*
