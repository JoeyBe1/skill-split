---
plan: 03-01
phase: 3 - Batch Embeddings
title: Implement True Batch Embedding Generation
status: ready
created: 2026-02-08
---

# Plan 03-01: Implement True Batch Embedding Generation

**Goal**: Large file embedding generation is 10-100x faster through optimized batch API calls and parallel processing

**Requirement**: GS-02 - Implement batch embedding generation

## Current State Analysis

**Existing Implementation:**
- `EmbeddingService.batch_generate()` exists (line 99-149) with max_batch_size=100
- OpenAI API supports up to **2,048** texts per request (not 100)
- `supabase_store.py:521` calls `generate_embedding()` individually for each section
- No parallel processing - all embeddings generated sequentially

**Performance Bottleneck:**
- 19,207 sections × 1 API call = 19,207 API calls
- At 3,000 RPM limit = 6.4 minutes minimum
- Real-world: 30+ minutes with failures/retries

## Success Criteria (Observable Truths)

1. **Batch size increased to 2,048**: `max_batch_size` parameter accepts up to 2048
2. **Parallel processing enabled**: Multiple batches processed concurrently using ThreadPoolExecutor
3. **Integration fixed**: `supabase_store.py` and `database.py` use batch operations instead of individual calls
4. **Graceful failure**: Partial results saved if rate limit exceeded or batch fails
5. **Memory caching**: Embeddings cached in-memory during batch processing to avoid redundant API calls
6. **Progress reporting**: Users see progress during large batch operations

## Implementation Tasks

### Task 1: Update EmbeddingService for Large Batches

**File**: `core/embedding_service.py`

**Changes:**
1. Update `batch_generate()` docstring to reflect 2048 limit
2. Change default `max_batch_size` from 100 to 2048
3. Add validation for max_batch_size ≤ 2048
4. Add token-aware batching: estimate tokens per batch and stay under 8191 token limit

**Code Changes:**
```python
# Line 99: Update method signature
def batch_generate(
    self,
    texts: List[str],
    max_batch_size: int = 2048,  # Changed from 100 to 2048
    max_tokens_per_batch: int = 8000  # Stay under 8191 token limit
) -> List[List[float]]:
    """
    Generate embeddings in batch for efficiency.

    OpenAI API supports up to 2048 texts per request.
    Also limited by 8191 tokens per request.

    Args:
        texts: List of text strings to embed
        max_batch_size: Max texts per API call (OpenAI limit: 2048)
        max_tokens_per_batch: Max tokens per batch (OpenAI limit: 8191)

    Returns:
        List of embedding vectors, same length as input
    """
    # Validate batch size
    if max_batch_size > 2048:
        raise ValueError("max_batch_size cannot exceed 2048 (OpenAI API limit)")

    # Token-aware batching implementation
    ...
```

### Task 2: Add Parallel Batch Processing

**File**: `core/embedding_service.py`

**New Method:**
```python
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Callable, Any

def batch_generate_parallel(
    self,
    texts: List[str],
    max_batch_size: int = 2048,
    max_workers: int = 5,
    progress_callback: Optional[Callable[[int, int], None]] = None
) -> List[List[float]]:
    """
    Generate embeddings in parallel batches for maximum speed.

    Processes multiple batches concurrently using ThreadPoolExecutor.
    Maximize OpenAI rate limits (3,000 RPM for text-embedding-3-small).

    Args:
        texts: List of text strings to embed
        max_batch_size: Max texts per API call (default: 2048)
        max_workers: Max concurrent API calls (default: 5)
        progress_callback: Optional callback(current, total) for progress updates

    Returns:
        List of embedding vectors, same length as input
    """
    if not texts:
        return []

    # Create batches with token awareness
    batches = self._create_token_aware_batches(texts, max_batch_size)

    embeddings = [None] * len(texts)
    completed = 0

    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all batch jobs
        future_to_batch = {
            executor.submit(self._process_batch, batch): batch
            for batch in batches
        }

        # Collect results as they complete
        for future in as_completed(future_to_batch):
            batch_info = future_to_batch[future]
            try:
                result = future.result()
                # Map results back to original indices
                for idx, embedding in zip(batch_info['indices'], result):
                    embeddings[idx] = embedding
                completed += len(batch_info['indices'])
                if progress_callback:
                    progress_callback(completed, len(texts))
            except Exception as e:
                # Store error for this batch, continue with others
                print(f"Warning: Batch failed: {str(e)}")
                for idx in batch_info['indices']:
                    embeddings[idx] = None  # Mark as failed

    return embeddings

def _create_token_aware_batches(
    self,
    texts: List[str],
    max_batch_size: int
) -> List[Dict[str, Any]]:
    """Create batches with token count awareness."""
    batches = []
    current_batch = []
    current_tokens = 0
    current_indices = []

    for i, text in enumerate(texts):
        text_tokens = self.estimate_tokens(text)

        # Check if adding this text would exceed limits
        if (len(current_batch) >= max_batch_size or
            current_tokens + text_tokens > 8000):
            # Start new batch
            batches.append({
                'texts': current_batch,
                'indices': current_indices,
                'tokens': current_tokens
            })
            current_batch = []
            current_tokens = 0
            current_indices = []

        current_batch.append(text)
        current_tokens += text_tokens
        current_indices.append(i)

    # Add final batch
    if current_batch:
        batches.append({
            'texts': current_batch,
            'indices': current_indices,
            'tokens': current_tokens
        })

    return batches

def _process_batch(self, batch_info: Dict[str, Any]) -> List[List[float]]:
    """Process a single batch and return embeddings."""
    return self.batch_generate(batch_info['texts'], max_batch_size=len(batch_info['texts']))
```

### Task 3: Update SupabaseStore Integration

**File**: `core/supabase_store.py`

**Current Problem:** Line 521 calls `generate_embedding()` individually

**Solution:** Add batch method that collects all content and processes in parallel

**New Method:**
```python
def batch_generate_embeddings(
    self,
    sections: List[Dict[str, Any]],
    embedding_service: EmbeddingService,
    force_regenerate: bool = False
) -> Dict[int, List[float]]:
    """
    Generate embeddings for multiple sections in batch.

    Args:
        sections: List of section dicts with id, content, content_hash
        embedding_service: EmbeddingService instance
        force_regenerate: If True, regenerate all embeddings

    Returns:
        Dict mapping section_id to embedding vector
    """
    # Filter sections that need embeddings
    sections_to_embed = []
    for section in sections:
        if force_regenerate or not self._has_embedding(section['id']):
            sections_to_embed.append(section)

    if not sections_to_embed:
        return {}

    # Progress callback
    def progress_callback(current: int, total: int):
        pct = (current / total) * 100
        print(f"Generating embeddings: {current}/{total} ({pct:.1f}%)", end='\r')

    # Collect texts and indices
    texts = [s['content'] for s in sections_to_embed]

    # Generate embeddings in parallel
    embeddings = embedding_service.batch_generate_parallel(
        texts,
        progress_callback=progress_callback
    )

    # Store results
    section_embeddings = {}
    for section, embedding in zip(sections_to_embed, embeddings):
        if embedding is not None:  # Skip failed embeddings
            self._store_embedding(section['id'], embedding)
            section_embeddings[section['id']] = embedding

    return section_embeddings
```

### Task 4: Update DatabaseStore for Consistency

**File**: `core/database.py`

Add same `batch_generate_embeddings()` method for local SQLite consistency

### Task 5: Add Rate Limit Handling

**File**: `core/embedding_service.py`

```python
import time
from typing import Optional

class RateLimitError(Exception):
    """Raised when OpenAI rate limit is exceeded."""
    pass

def batch_generate_with_retry(
    self,
    texts: List[str],
    max_retries: int = 3,
    backoff_base: float = 1.0
) -> List[List[float]]:
    """
    Generate embeddings with automatic retry on rate limit.

    Implements exponential backoff for rate limit errors.

    Args:
        texts: List of text strings to embed
        max_retries: Max retry attempts per batch
        backoff_base: Base for exponential backoff (seconds)

    Returns:
        List of embedding vectors, same length as input
    """
    embeddings = []
    batches = self._create_token_aware_batches(texts, 2048)

    for batch in batches:
        retries = 0
        while retries < max_retries:
            try:
                result = self._process_batch(batch)
                embeddings.extend(result)
                break
            except Exception as e:
                if 'rate_limit' in str(e).lower() and retries < max_retries - 1:
                    wait_time = backoff_base * (2 ** retries)
                    print(f"Rate limited. Waiting {wait_time:.1f}s...")
                    time.sleep(wait_time)
                    retries += 1
                else:
                    raise

    return embeddings
```

## Testing Requirements

1. Unit tests for `_create_token_aware_batches()`
2. Unit tests for `batch_generate_parallel()`
3. Integration test for `batch_generate_embeddings()`
4. Test rate limit handling
5. Test partial failure scenarios
6. Test progress callback

## Verification Checklist

- [ ] `max_batch_size` accepts up to 2048
- [ ] Token-aware batching implemented
- [ ] Parallel processing with ThreadPoolExecutor
- [ ] `supabase_store.py` uses batch operations
- [ ] `database.py` has consistent batch method
- [ ] Rate limit retry with exponential backoff
- [ ] Progress callback for user feedback
- [ ] Graceful failure (partial results saved)
- [ ] All existing tests pass

## Dependencies

- Depends on: Nothing (can run independently)
- Blocks: Plan 03-02 (testing depends on implementation)

## Estimated Duration

30-45 minutes

---

*Created: 2026-02-08*
*Status: Ready for execution*
