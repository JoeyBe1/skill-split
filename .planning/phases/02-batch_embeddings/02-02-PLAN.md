---
phase: 02-batch_embeddings
plan: 02
type: tdd
wave: 2
depends_on: ["02-01"]
files_modified:
  - test/test_embedding_service.py
  - test/test_supabase_store.py
autonomous: true

must_haves:
  truths:
    - "User ingests 1000 sections and embeddings complete in ~1 minute instead of ~15 minutes"
    - "Embedding service automatically batches requests up to OpenAI's limit (2048 embeddings per batch)"
    - "Batch embedding fails gracefully with partial results if API rate limit exceeded"
    - "All existing embedding tests pass with batch implementation"
  artifacts:
    - path: "test/test_embedding_service.py"
      provides: "Tests for batch_generate_embeddings() with 2048-item batches, graceful failure, and performance benchmarks"
      contains: "test_batch_generate_embeddings"
      min_lines: 120
    - path: "test/test_supabase_store.py"
      provides: "Tests for batch_ingest_embeddings() integration with Supabase storage"
      contains: "test_batch_ingest_embeddings"
      min_lines: 80
  key_links:
    - from: "test/test_embedding_service.py::test_batch_generate_embeddings_2048"
      to: "core/embedding_service.py:batch_generate_embeddings()"
      via: "Unit test verifying 2048-item batch processing"
      pattern: "batch_generate_embeddings.*2048"
    - from: "test/test_embedding_service.py::test_batch_generate_embeddings_partial_failure"
      to: "core/embedding_service.py:batch_generate_embeddings()"
      via: "Unit test verifying graceful failure with partial results"
      pattern: "failed_indices|partial"
    - from: "test/test_supabase_store.py::test_batch_ingest_embeddings_integration"
      to: "core/supabase_store.py:batch_ingest_embeddings()"
      via: "Integration test verifying batch ingestion with storage"
      pattern: "batch_ingest_embeddings"
    - from: "test/test_embedding_service.py::test_batch_performance_benchmark"
      to: "production performance"
      via: "Performance test demonstrating 10-100x speedup"
      pattern: "performance|benchmark|speedup"
---

<objective>
Add comprehensive tests for batch embedding functionality including 2048-item batch processing, graceful failure handling, Supabase integration, and performance benchmarks demonstrating 10-100x speedup.

Purpose: Verify that batch_generate_embeddings() correctly handles large batches (2048 items), returns partial results on failure, and provides the expected performance improvement. Integration tests confirm SupabaseStore.batch_ingest_embeddings() correctly stores batch results. Performance benchmarks document the actual speedup achieved.

Output: Comprehensive test suite for batch embedding functionality with TDD approach (RED-GREEN-REFACTOR), including unit tests, integration tests, and performance benchmarks.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/codebase/ARCHITECTURE.md

@test/test_embedding_service.py
@test/test_supabase_store.py
@core/embedding_service.py
@core/supabase_store.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add batch_generate_embeddings() unit tests (RED)</name>
  <files>test/test_embedding_service.py</files>
  <action>
    Add comprehensive unit tests for the new batch_generate_embeddings() method:

    Add new test class after TestBatchGeneration class (around line 148):
    ```python
    class TestBatchGenerateEmbeddings:
        """Test new batch embedding generation with 2048-item batches."""

        @patch('core.embedding_service.OpenAI')
        def test_batch_generate_embeddings_success_2048(self, mock_openai):
            """Test successful batch embedding with 2048 items (OpenAI's limit)."""
            # Create mock response for 2048 embeddings
            mock_embedding = [0.1] * 1536
            mock_response = MagicMock()

            # Simulate 2048 embeddings in response
            mock_response.data = [MagicMock(embedding=mock_embedding) for _ in range(2048)]
            mock_response.usage = MagicMock(prompt_tokens=100000)

            mock_client = MagicMock()
            mock_client.embeddings.create.return_value = mock_response
            mock_openai.return_value = mock_client

            service = EmbeddingService(api_key="test-key")

            # Generate 2048 embeddings
            texts = [f"text{i}" for i in range(2048)]
            result = service.batch_generate_embeddings(texts)

            # Verify structure
            assert 'embeddings' in result
            assert 'failed_indices' in result
            assert 'total_tokens' in result

            # Verify all embeddings generated
            assert len(result['embeddings']) == 2048
            assert len(result['failed_indices']) == 0
            assert result['total_tokens'] == 100000

            # Verify single API call made (2048 items fit in one batch)
            assert mock_client.embeddings.create.call_count == 1

        @patch('core.embedding_service.OpenAI')
        def test_batch_generate_embeddings_chunking(self, mock_openai):
            """Test automatic chunking for datasets larger than batch size."""
            mock_embedding = [0.1] * 1536

            mock_client = MagicMock()

            # Create responses for multiple chunks
            def mock_create(**kwargs):
                batch_size = len(kwargs['input'])
                mock_response = MagicMock()
                mock_response.data = [MagicMock(embedding=mock_embedding) for _ in range(batch_size)]
                mock_response.usage = MagicMock(prompt_tokens=batch_size * 10)
                return mock_response

            mock_client.embeddings.create.side_effect = mock_create
            mock_openai.return_value = mock_client

            service = EmbeddingService(api_key="test-key")

            # Generate 5000 embeddings (should chunk into 3 batches: 2048 + 2048 + 904)
            texts = [f"text{i}" for i in range(5000)]
            result = service.batch_generate_embeddings(texts)

            # Verify all embeddings generated
            assert len(result['embeddings']) == 5000
            assert len(result['failed_indices']) == 0

            # Verify 3 API calls made (chunking)
            assert mock_client.embeddings.create.call_count == 3

        @patch('core.embedding_service.OpenAI')
        def test_batch_generate_embeddings_partial_failure(self, mock_openai):
            """Test graceful failure with partial results."""
            mock_embedding = [0.1] * 1536

            mock_client = MagicMock()

            # First call succeeds, second call fails, third call succeeds
            call_count = [0]

            def mock_create_with_failure(**kwargs):
                call_count[0] += 1
                if call_count[0] == 2:
                    # Second batch fails
                    raise Exception("API rate limit exceeded")

                batch_size = len(kwargs['input'])
                mock_response = MagicMock()
                mock_response.data = [MagicMock(embedding=mock_embedding) for _ in range(batch_size)]
                mock_response.usage = MagicMock(prompt_tokens=batch_size * 10)
                return mock_response

            mock_client.embeddings.create.side_effect = mock_create_with_failure
            mock_openai.return_value = mock_client

            service = EmbeddingService(api_key="test-key")

            # Generate 3000 embeddings (3 batches, middle one fails)
            texts = [f"text{i}" for i in range(3000)]
            result = service.batch_generate_embeddings(texts)

            # Verify partial results: first 2048 + last 952 = 3000
            # Actually middle batch fails, so we get first 2048 + last 952
            assert len(result['embeddings']) > 0  # Some succeeded
            assert len(result['embeddings']) < 3000  # Not all succeeded

            # Failed indices should be recorded
            assert len(result['failed_indices']) > 0
            assert all(isinstance(idx, int) for idx in result['failed_indices'])

            # At least some embeddings succeeded
            assert len(result['embeddings']) + len(result['failed_indices']) >= 3000

        @patch('core.embedding_service.OpenAI')
        def test_batch_generate_embeddings_empty_list(self, mock_openai):
            """Test batch generation rejects empty list."""
            mock_openai.return_value = MagicMock()

            service = EmbeddingService(api_key="test-key")

            with pytest.raises(ValueError, match="empty list"):
                service.batch_generate_embeddings([])

        @patch('core.embedding_service.OpenAI')
        def test_batch_generate_embeddings_empty_text(self, mock_openai):
            """Test batch generation rejects empty strings."""
            mock_openai.return_value = MagicMock()

            service = EmbeddingService(api_key="test-key")

            with pytest.raises(ValueError, match="empty"):
                service.batch_generate_embeddings(["text1", "", "text3"])

        @patch('core.embedding_service.OpenAI')
        def test_batch_generate_embeddings_all_fail(self, mock_openai):
            """Test complete failure raises error."""
            mock_client = MagicMock()
            mock_client.embeddings.create.side_effect = Exception("All API calls failed")
            mock_openai.return_value = mock_client

            service = EmbeddingService(api_key="test-key")

            with pytest.raises(RuntimeError, match="Failed to generate any embeddings"):
                service.batch_generate_embeddings(["text1", "text2"])

        @patch('core.embedding_service.OpenAI')
        def test_batch_generate_embeddings_custom_batch_size(self, mock_openai):
            """Test custom batch size parameter."""
            mock_embedding = [0.1] * 1536
            mock_response = MagicMock()
            mock_response.data = [MagicMock(embedding=mock_embedding) for _ in range(100)]
            mock_response.usage = MagicMock(prompt_tokens=1000)

            mock_client = MagicMock()
            mock_client.embeddings.create.return_value = mock_response
            mock_openai.return_value = mock_client

            service = EmbeddingService(api_key="test-key")

            # Use custom batch size
            texts = [f"text{i}" for i in range(300)]
            result = service.batch_generate_embeddings(texts, max_batch_size=100)

            # Should make 3 API calls with custom batch size
            assert len(result['embeddings']) == 300
            assert mock_client.embeddings.create.call_count == 3
    ```

    Why: TDD RED phase - these tests describe the expected behavior of batch_generate_embeddings(). Tests cover success cases (2048 items, chunking), failure cases (partial failure, complete failure), and edge cases (empty inputs, custom batch size).

    Running these tests before implementation will fail (RED), confirming we're testing the right behavior.
  </action>
  <verify>
    Run: `python -m pytest test/test_embedding_service.py::TestBatchGenerateEmbeddings -v`
    Expected: Tests fail (RED phase) - batch_generate_embeddings() not yet implemented or incomplete
  </verify>
  <done>Test suite for batch_generate_embeddings() written with comprehensive coverage of success, failure, and edge cases. Tests fail as expected in RED phase.</done>
</task>

<task type="auto">
  <name>Task 2: Implement batch_generate_embeddings() (GREEN)</name>
  <files>core/embedding_service.py</files>
  <action>
    Implement the batch_generate_embeddings() method to make tests pass:

    (This is the GREEN phase - implementation already exists from 02-01-PLAN.md Task 1,
     but we're confirming it makes all tests pass)

    The implementation should:
    1. Accept list of texts and optional max_batch_size parameter
    2. Validate inputs (no empty lists or strings)
    3. Process in chunks of max_batch_size (default 2048)
    4. Call OpenAI embeddings API for each chunk
    5. Handle exceptions gracefully (continue on failure)
    6. Return dict with embeddings, failed_indices, total_tokens
    7. Raise RuntimeError if all batches fail

    Key implementation details from 02-01-PLAN.md:
    - Use MAX_BATCH_SIZE = 2048 constant
    - Track token usage across all batches
    - Return partial results on failure
    - Maintain all_embeddings list with None for failed items
    - Remove None values from successful_embeddings

    Why: GREEN phase - implement the minimum code to make tests pass. The implementation
    from 02-01-PLAN.md Task 1 should already satisfy these tests.

    If tests fail, debug and fix implementation until all tests pass.
  </action>
  <verify>
    Run: `python -m pytest test/test_embedding_service.py::TestBatchGenerateEmbeddings -v`
    Expected: All tests pass (GREEN phase)
    Run: `python -m pytest test/test_embedding_service.py -v`
    Expected: All existing tests still pass (no regressions)
  </verify>
  <done>batch_generate_embeddings() implemented and all tests passing. Method correctly handles 2048-item batches, chunking, partial failures, and edge cases.</done>
</task>

<task type="auto">
  <name>Task 3: Add batch_ingest_embeddings() integration tests (RED)</name>
  <files>test/test_supabase_store.py</files>
  <action>
    Add integration tests for batch_ingest_embeddings() method:

    Add new test class (find appropriate location in file, after existing ingestion tests):
    ```python
    class TestBatchIngestEmbeddings:
        """Test batch embedding ingestion integration."""

        @patch('core.supabase_store.datetime')
        @patch('core.embedding_service.OpenAI')
        def test_batch_ingest_embeddings_success(self, mock_openai, mock_datetime, mock_supabase_store):
            """Test successful batch ingestion."""
            # Setup embedding service mock
            mock_embedding = [0.1] * 1536
            mock_response = MagicMock()
            mock_response.data = [MagicMock(embedding=mock_embedding) for _ in range(10)]
            mock_response.usage = MagicMock(prompt_tokens=100)

            mock_client = MagicMock()
            mock_client.embeddings.create.return_value = mock_response
            mock_openai.return_value = mock_client

            from core.embedding_service import EmbeddingService
            embedding_service = EmbeddingService(api_key="test-key")

            # Setup sections
            sections = [
                {'id': i, 'title': f'Section {i}', 'content': f'Content {i}'}
                for i in range(1, 11)
            ]

            # Mock Supabase upsert
            mock_supabase_store.client.table().upsert().execute.return_value = MagicMock()
            mock_datetime.now.return_value.isoformat.return_value = "2026-02-08T00:00:00"

            # Ingest embeddings
            result = mock_supabase_store.batch_ingest_embeddings(
                sections, embedding_service, batch_size=2048
            )

            # Verify result structure
            assert 'success_count' in result
            assert 'failed_count' in result
            assert 'total_tokens' in result
            assert 'failed_ids' in result

            # Verify all succeeded
            assert result['success_count'] == 10
            assert result['failed_count'] == 0
            assert result['total_tokens'] == 100
            assert len(result['failed_ids']) == 0

            # Verify Supabase upsert called 10 times
            assert mock_supabase_store.client.table().upsert.call_count == 10

        @patch('core.supabase_store.datetime')
        @patch('core.embedding_service.OpenAI')
        def test_batch_ingest_embeddings_empty_list(self, mock_openai, mock_datetime, mock_supabase_store):
            """Test batch ingestion with empty section list."""
            mock_openai.return_value = MagicMock()

            from core.embedding_service import EmbeddingService
            embedding_service = EmbeddingService(api_key="test-key")

            result = mock_supabase_store.batch_ingest_embeddings(
                [], embedding_service
            )

            # Should return empty result without errors
            assert result['success_count'] == 0
            assert result['failed_count'] == 0
            assert result['total_tokens'] == 0
            assert result['failed_ids'] == []

        @patch('core.supabase_store.datetime')
        @patch('core.embedding_service.OpenAI')
        def test_batch_ingest_embeddings_partial_failure(self, mock_openai, mock_datetime, mock_supabase_store):
            """Test batch ingestion with some failures."""
            # Setup embedding service to return partial results
            mock_embedding = [0.1] * 1536

            mock_client = MagicMock()
            mock_response = MagicMock()
            # Return only 8 embeddings (2 failed)
            mock_response.data = [MagicMock(embedding=mock_embedding) for _ in range(8)]
            mock_response.usage = MagicMock(prompt_tokens=80)
            mock_client.embeddings.create.return_value = mock_response
            mock_openai.return_value = mock_client

            from core.embedding_service import EmbeddingService
            embedding_service = EmbeddingService(api_key="test-key")

            sections = [
                {'id': i, 'title': f'Section {i}', 'content': f'Content {i}'}
                for i in range(1, 11)
            ]

            # Mock upsert that fails for some sections
            def mock_upsert_side_effect(data, **kwargs):
                # Fail for section IDs 3 and 7
                if data['section_id'] in [3, 7]:
                    raise Exception("Database error")
                return MagicMock(execute=MagicMock())

            mock_supabase_store.client.table().upsert.side_effect = mock_upsert_side_effect
            mock_datetime.now.return_value.isoformat.return_value = "2026-02-08T00:00:00"

            result = mock_supabase_store.batch_ingest_embeddings(
                sections, embedding_service
            )

            # Verify partial success
            assert result['success_count'] == 8  # 8 succeeded
            assert result['failed_count'] == 2  # 2 failed
            assert 3 in result['failed_ids'] or 7 in result['failed_ids']

        @patch('core.supabase_store.datetime')
        @patch('core.embedding_service.OpenAI')
        def test_batch_ingest_embeddings_large_dataset(self, mock_openai, mock_datetime, mock_supabase_store):
            """Test batch ingestion with 1000+ sections."""
            # Mock large batch response
            mock_embedding = [0.1] * 1536

            mock_client = MagicMock()
            mock_response = MagicMock()
            mock_response.data = [MagicMock(embedding=mock_embedding) for _ in range(1000)]
            mock_response.usage = MagicMock(prompt_tokens=10000)
            mock_client.embeddings.create.return_value = mock_response
            mock_openai.return_value = mock_client

            from core.embedding_service import EmbeddingService
            embedding_service = EmbeddingService(api_key="test-key")

            sections = [
                {'id': i, 'title': f'Section {i}', 'content': f'Content {i}'}
                for i in range(1, 1001)
            ]

            mock_supabase_store.client.table().upsert().execute.return_value = MagicMock()
            mock_datetime.now.return_value.isoformat.return_value = "2026-02-08T00:00:00"

            result = mock_supabase_store.batch_ingest_embeddings(
                sections, embedding_service, batch_size=2048
            )

            # All 1000 should succeed
            assert result['success_count'] == 1000
            assert result['failed_count'] == 0
            assert result['total_tokens'] == 10000

            # Verify single API call (1000 < 2048)
            assert mock_client.embeddings.create.call_count == 1
    ```

    Why: TDD RED phase for integration tests. These tests verify that batch_ingest_embeddings()
    correctly integrates with EmbeddingService and Supabase storage. Tests cover success cases,
    partial failures, empty inputs, and large datasets.

    Tests use mocks to avoid real API calls and database operations.
  </action>
  <verify>
    Run: `python -m pytest test/test_supabase_store.py::TestBatchIngestEmbeddings -v`
    Expected: Tests fail (RED phase) - batch_ingest_embeddings() not yet implemented or incomplete
  </verify>
  <done>Integration test suite for batch_ingest_embeddings() written. Tests cover success, partial failure, empty inputs, and large datasets. Tests fail as expected in RED phase.</done>
</task>

<task type="auto">
  <name>Task 4: Implement batch_ingest_embeddings() (GREEN)</name>
  <files>core/supabase_store.py</files>
  <action>
    Implement the batch_ingest_embeddings() method to make tests pass:

    (This is the GREEN phase - implementation already exists from 02-01-PLAN.md Task 2,
     but we're confirming it makes all tests pass)

    The implementation should:
    1. Accept list of section dicts and EmbeddingService instance
    2. Prepare texts by combining title and content
    3. Call embedding_service.batch_generate_embeddings()
    4. Store successful embeddings in Supabase
    5. Handle storage failures gracefully
    6. Return dict with success_count, failed_count, total_tokens, failed_ids

    Key implementation details from 02-01-PLAN.md Task 2:
    - Combine title and content for better embeddings
    - Map section IDs to embeddings correctly
    - Handle partial failures from batch_generate_embeddings()
    - Return clear success/failure reporting

    If tests fail, debug and fix implementation until all tests pass.

    Common issues to check:
    - datetime import at top of file
    - Correct dict keys in return value
    - Proper exception handling
    - Section ID mapping (index vs actual ID)
  </action>
  <verify>
    Run: `python -m pytest test/test_supabase_store.py::TestBatchIngestEmbeddings -v`
    Expected: All tests pass (GREEN phase)
    Run: `python -m pytest test/test_supabase_store.py -v`
    Expected: All existing tests still pass (no regressions)
  </verify>
  <done>batch_ingest_embeddings() implemented and all integration tests passing. Method correctly integrates batch embedding generation with Supabase storage.</done>
</task>

<task type="auto">
  <name>Task 5: Add performance benchmark tests</name>
  <files>test/test_embedding_service.py</files>
  <action>
    Add performance benchmark tests to document the speedup:

    Add new test class at end of file:
    ```python
    class TestBatchPerformanceBenchmarks:
        """Performance benchmarks for batch embedding generation."""

        @patch('core.embedding_service.OpenAI')
        def test_batch_vs_individual_performance(self, mock_openai):
            """Benchmark: batch generation is significantly faster than individual."""
            import time

            # Setup mock
            mock_embedding = [0.1] * 1536

            mock_client = MagicMock()

            def mock_create_with_delay(**kwargs):
                # Simulate API latency (1ms per call regardless of batch size)
                time.sleep(0.001)
                batch_size = len(kwargs['input'])
                mock_response = MagicMock()
                mock_response.data = [MagicMock(embedding=mock_embedding) for _ in range(batch_size)]
                mock_response.usage = MagicMock(prompt_tokens=batch_size * 10)
                return mock_response

            mock_client.embeddings.create.side_effect = mock_create_with_delay
            mock_openai.return_value = mock_client

            from core.embedding_service import EmbeddingService
            service = EmbeddingService(api_key="test-key")

            # Benchmark individual generation (100 items)
            texts = [f"text{i}" for i in range(100)]

            start_individual = time.time()
            individual_results = []
            for text in texts:
                result = service.generate_embedding(text)
                individual_results.append(result)
            time_individual = time.time() - start_individual

            # Benchmark batch generation (100 items)
            start_batch = time.time()
            batch_result = service.batch_generate_embeddings(texts)
            time_batch = time.time() - start_batch

            # Batch should be significantly faster (at least 10x with mocked latency)
            speedup = time_individual / time_batch
            print(f"\\nPerformance: {speedup:.1f}x speedup (batch vs individual)")
            print(f"Individual: {time_individual:.3f}s, Batch: {time_batch:.3f}s")

            # Assert reasonable speedup (at least 5x with mocked latency)
            assert speedup >= 5, f"Batch should be at least 5x faster, got {speedup:.1f}x"

        @patch('core.embedding_service.OpenAI')
        def test_large_dataset_performance(self, mock_openai):
            """Benchmark: process 1000 sections efficiently."""
            import time

            mock_embedding = [0.1] * 1536

            mock_client = MagicMock()
            mock_response = MagicMock()
            mock_response.data = [MagicMock(embedding=mock_embedding) for _ in range(1000)]
            mock_response.usage = MagicMock(prompt_tokens=10000)
            mock_client.embeddings.create.return_value = mock_response
            mock_openai.return_value = mock_client

            from core.embedding_service import EmbeddingService
            service = EmbeddingService(api_key="test-key")

            # Process 1000 embeddings
            texts = [f"section content {i}" for i in range(1000)]

            start = time.time()
            result = service.batch_generate_embeddings(texts)
            elapsed = time.time() - start

            # Verify all succeeded
            assert len(result['embeddings']) == 1000
            assert len(result['failed_indices']) == 0

            print(f"\\n1000 embeddings processed in {elapsed:.3f}s")
            print(f"API calls made: {mock_client.embeddings.create.call_count}")

            # Should be single API call (1000 < 2048)
            assert mock_client.embeddings.create.call_count == 1

        @patch('core.embedding_service.OpenAI')
        def test_2048_batch_limit(self, mock_openai):
            """Benchmark: verify 2048 is the optimal batch size."""
            mock_embedding = [0.1] * 1536

            mock_client = MagicMock()
            mock_response = MagicMock()

            def mock_create(**kwargs):
                batch_size = len(kwargs['input'])
                mock_response = MagicMock()
                mock_response.data = [MagicMock(embedding=mock_embedding) for _ in range(batch_size)]
                mock_response.usage = MagicMock(prompt_tokens=batch_size * 10)
                return mock_response

            mock_client.embeddings.create.side_effect = mock_create
            mock_openai.return_value = mock_client

            from core.embedding_service import EmbeddingService
            service = EmbeddingService(api_key="test-key")

            # Test exactly 2048 items
            texts = [f"text{i}" for i in range(2048)]
            result = service.batch_generate_embeddings(texts)

            assert len(result['embeddings']) == 2048
            assert len(result['failed_indices']) == 0
            assert mock_client.embeddings.create.call_count == 1  # Single call

            # Test 2049 items (should chunk into 2 calls)
            mock_client.embeddings.create.reset_mock()
            texts = [f"text{i}" for i in range(2049)]
            result = service.batch_generate_embeddings(texts)

            assert len(result['embeddings']) == 2049
            assert mock_client.embeddings.create.call_count == 2  # Two calls
    ```

    Why: Performance benchmarks document the actual speedup achieved by batch processing.
    Tests demonstrate:
    - Batch vs individual performance comparison
    - Large dataset processing (1000 items)
    - Verification that 2048 is optimal batch size

    Note: These use mocked API latency (1ms per call) to demonstrate speedup without
    making actual API calls. Real-world speedup will be even more significant due to
    OpenAI's actual API latency (~500ms per call).
  </action>
  <verify>
    Run: `python -m pytest test/test_embedding_service.py::TestBatchPerformanceBenchmarks -v -s`
    Expected: All tests pass, performance metrics printed
    Expected output: Shows speedup multiplier and timing comparisons
  </verify>
  <done>Performance benchmark tests added and passing. Benchmarks demonstrate significant speedup for batch processing. 2048 confirmed as optimal batch size.</done>
</task>

</tasks>

<verification>
Overall verification after all tasks complete:
1. Run all embedding tests: `python -m pytest test/test_embedding_service.py -v`
   Expected: All tests pass (existing + new)
2. Run all Supabase tests: `python -m pytest test/test_supabase_store.py -v`
   Expected: All tests pass (existing + new)
3. Run performance benchmarks with output: `python -m pytest test/test_embedding_service.py::TestBatchPerformanceBenchmarks -v -s`
   Expected: Benchmarks print timing and speedup metrics
4. Verify TDD cycle complete:
   ```bash
   # Confirm RED-GREEN-REFACTOR cycle completed
   git log --oneline -5
   ```
   Expected: Commits for test addition, implementation, and any refactoring
5. Check test coverage: `python -m pytest test/test_embedding_service.py test/test_supabase_store.py --cov=core/embedding_service --cov=core/supabase_store --cov-report=term-missing`
   Expected: >90% coverage for new methods
</verification>

<success_criteria>
1. TestBatchGenerateEmbeddings class with 7 comprehensive tests (success, chunking, partial failure, edge cases)
2. TestBatchIngestEmbeddings class with 4 integration tests (success, empty, partial failure, large dataset)
3. TestBatchPerformanceBenchmarks class with 3 benchmark tests (batch vs individual, large dataset, 2048 limit)
4. All new tests pass (GREEN phase)
5. All existing tests continue to pass (no regressions)
6. Performance benchmarks demonstrate 10-100x speedup
7. Test coverage >90% for new methods
8. TDD cycle documented in commits (test -> implement -> refactor)
</success_criteria>

<output>
After completion, create `.planning/phases/02-batch_embeddings/02-02-SUMMARY.md` with:
- Tests added (7 unit tests, 4 integration tests, 3 benchmarks)
- TDD cycle confirmation (RED -> GREEN -> REFACTOR)
- Performance benchmark results (speedup metrics, timing comparisons)
- Test coverage metrics
- Edge cases discovered and tested
- Verification results (all tests passing)
</output>
