---
phase: 02-batch_embeddings
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - core/embedding_service.py
  - core/supabase_store.py
autonomous: true

must_haves:
  truths:
    - "User ingests 1000 sections and embeddings complete in ~1 minute instead of ~15 minutes"
    - "Embedding service automatically batches requests up to OpenAI's limit (2048 embeddings per batch)"
    - "Batch embedding fails gracefully with partial results if API rate limit exceeded"
    - "All existing embedding tests pass with batch implementation"
  artifacts:
    - path: "core/embedding_service.py"
      provides: "Batch embedding generation with 2048 embeddings per batch, automatic chunking, and graceful failure handling"
      contains: "batch_generate_embeddings"
      min_lines: 80
    - path: "core/supabase_store.py"
      provides: "Updated batch embedding ingestion using new batch API"
      contains: "batch_ingest_embeddings"
      min_lines: 40
  key_links:
    - from: "core/supabase_store.py:batch_ingest_embeddings()"
      to: "core/embedding_service.py:batch_generate_embeddings()"
      via: "Method call to batch embedding service with automatic chunking"
      pattern: "batch_generate_embeddings"
    - from: "core/embedding_service.py:batch_generate_embeddings()"
      to: "OpenAI embeddings API"
      via: "Batch API calls with up to 2048 embeddings per request"
      pattern: "client\.embeddings\.create.*input=.*batch"
    - from: "core/embedding_service.py:batch_generate_embeddings()"
      to: "Error handling"
      via: "Partial results returned on API failures"
      pattern: "except.*Exception|partial.*results"
---

<objective>
Refactor EmbeddingService to support batch embedding generation with 10-100x speedup for large-scale ingestion operations.

Purpose: Current embedding generation processes sections individually (~1 second per section = ~15 minutes for 1000 sections). OpenAI's batch embedding API supports up to 2048 embeddings per request, enabling 10-100x speedup. The batch implementation must handle rate limits gracefully and return partial results on failure.

Output: EmbeddingService with batch_generate_embeddings() method supporting 2048 embeddings per batch, automatic chunking for large datasets, and graceful failure handling. SupabaseStore updated to use batch API for ingestion.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/codebase/ARCHITECTURE.md

@core/embedding_service.py
@core/supabase_store.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add batch_generate_embeddings() to EmbeddingService</name>
  <files>core/embedding_service.py</files>
  <action>
    Add new batch embedding method that processes up to 2048 embeddings per API call:

    Add new constant at class level (after line 27):
    ```python
    # OpenAI batch API limits
    MAX_BATCH_SIZE = 2048  # Maximum embeddings per API call
    ```

    Add new method after batch_generate() method (after line 149):
    ```python
    def batch_generate_embeddings(
        self,
        texts: List[str],
        max_batch_size: int = MAX_BATCH_SIZE
    ) -> Dict[str, Any]:
        """
        Generate embeddings in large batches for maximum efficiency.

        OpenAI supports up to 2048 embeddings per API call. This method
        automatically chunks large datasets and returns partial results on
        failure to enable graceful degradation.

        Args:
            texts: List of text strings to embed (can be 10,000+ items)
            max_batch_size: Max texts per API call (default: 2048, OpenAI's limit)

        Returns:
            Dictionary with:
                - 'embeddings': List of embedding vectors (len = successful embeddings)
                - 'failed_indices': List of indices that failed (empty on full success)
                - 'total_tokens': Total tokens used across all batches

        Raises:
            ValueError: If texts list is empty or contains empty strings
            RuntimeError: If all API calls fail
        """
        if not texts:
            raise ValueError("Cannot generate embeddings for empty list")

        # Validate inputs
        for i, text in enumerate(texts):
            if not text or not text.strip():
                raise ValueError(f"Text at index {i} is empty")

        all_embeddings = [None] * len(texts)
        failed_indices = []
        total_tokens = 0

        # Process in chunks of max_batch_size
        for chunk_start in range(0, len(texts), max_batch_size):
            chunk_end = min(chunk_start + max_batch_size, len(texts))
            chunk = texts[chunk_start:chunk_end]
            chunk_indices = list(range(chunk_start, chunk_end))

            try:
                # Clean whitespace for API call
                clean_batch = [" ".join(text.split()) for text in chunk]

                # Call OpenAI batch embedding API
                response = self.client.embeddings.create(
                    model=self.model,
                    input=clean_batch
                )

                # Track token usage
                if hasattr(response, 'usage'):
                    total_tokens += response.usage.prompt_tokens

                # Store embeddings in correct positions
                for i, item in enumerate(response.data):
                    all_embeddings[chunk_indices[i]] = item.embedding

            except Exception as e:
                # Mark all indices in this chunk as failed
                failed_indices.extend(chunk_indices)
                print(f"Warning: Batch embedding failed for indices {chunk_start}-{chunk_end}: {str(e)}")

                # Continue with next chunk (graceful degradation)
                continue

        # Remove None values (failed embeddings) from results
        successful_embeddings = [emb for emb in all_embeddings if emb is not None]

        # If everything failed, raise error
        if not successful_embeddings:
            raise RuntimeError(f"Failed to generate any embeddings. All batches failed.")

        return {
            'embeddings': successful_embeddings,
            'failed_indices': failed_indices,
            'total_tokens': total_tokens
        }
    ```

    Why: The new method uses OpenAI's actual batch limit (2048) instead of the current 100-item limit. This provides 20x more efficiency per API call. Graceful failure handling ensures partial results are returned even if some batches fail, enabling incremental progress on large ingestion jobs.

    Key differences from existing batch_generate():
    - Uses OpenAI's actual limit (2048 vs 100)
    - Returns dict with embeddings, failed_indices, and token_usage
    - Continues processing on failure (graceful degradation)
    - Handles 10,000+ item datasets efficiently
  </action>
  <verify>
    1. Check syntax: `python -c "from core.embedding_service import EmbeddingService; print('Import successful')"`
       Expected: No import errors
    2. Verify method exists:
       ```python
       from core.embedding_service import EmbeddingService
       assert hasattr(EmbeddingService, 'batch_generate_embeddings')
       assert EmbeddingService.MAX_BATCH_SIZE == 2048
       ```
       Expected: Method exists, MAX_BATCH_SIZE = 2048
    3. Test with mock (should return dict with correct keys):
       ```python
       from unittest.mock import Mock, patch
       with patch('core.embedding_service.OpenAI'):
           service = EmbeddingService(api_key="test")
           # Mock successful response
           # Call method and verify return structure
       ```
       Expected: Returns dict with 'embeddings', 'failed_indices', 'total_tokens' keys
  </verify>
  <done>EmbeddingService has batch_generate_embeddings() method supporting 2048 embeddings per batch with graceful failure handling and partial results.</done>
</task>

<task type="auto">
  <name>Task 2: Update SupabaseStore to use batch embedding API</name>
  <files>core/supabase_store.py</files>
  <action>
    Add batch embedding ingestion method that uses the new batch API:

    Find the ingest_embeddings method and add a new batch method after it (around line 200-250, based on file structure):
    ```python
    def batch_ingest_embeddings(
        self,
        sections: List[Dict[str, Any]],
        embedding_service: 'EmbeddingService',
        batch_size: int = 2048
    ) -> Dict[str, Any]:
        """
        Ingest embeddings for multiple sections in batches for efficiency.

        Uses batch_generate_embeddings to process up to 2048 sections per API call,
        providing 10-100x speedup over individual embedding generation.

        Args:
            sections: List of section dicts with 'id', 'title', 'content' keys
            embedding_service: EmbeddingService instance
            batch_size: Number of embeddings per batch (default: 2048)

        Returns:
            Dictionary with:
                - 'success_count': Number of embeddings successfully generated
                - 'failed_count': Number of embeddings that failed
                - 'total_tokens': Total tokens used
                - 'failed_ids': List of section IDs that failed
        """
        if not sections:
            return {
                'success_count': 0,
                'failed_count': 0,
                'total_tokens': 0,
                'failed_ids': []
            }

        # Prepare texts for batch embedding
        texts = []
        section_ids = []
        for section in sections:
            # Combine title and content for better embeddings
            text = f"{section.get('title', '')}\n{section.get('content', '')}"
            texts.append(text)
            section_ids.append(section['id'])

        # Generate embeddings in batches
        result = embedding_service.batch_generate_embeddings(texts, max_batch_size=batch_size)

        # Store successful embeddings
        success_count = 0
        failed_count = 0
        failed_ids = []

        for i, embedding in enumerate(result['embeddings']):
            if i < len(section_ids) and embedding is not None:
                try:
                    self.client.table("section_embeddings").upsert({
                        "section_id": section_ids[i],
                        "embedding": embedding,
                        "model_name": embedding_service.model,
                        "created_at": datetime.now().isoformat()
                    }, on_conflict="section_id,model_name").execute()
                    success_count += 1
                except Exception as e:
                    print(f"Warning: Failed to store embedding for section {section_ids[i]}: {str(e)}")
                    failed_count += 1
                    failed_ids.append(section_ids[i])

        # Handle failed indices from batch generation
        for idx in result['failed_indices']:
            if idx < len(section_ids):
                failed_count += 1
                failed_ids.append(section_ids[idx])

        return {
            'success_count': success_count,
            'failed_count': failed_count,
            'total_tokens': result['total_tokens'],
            'failed_ids': failed_ids
        }
    ```

    Also add datetime import at top if not present:
    ```python
    from datetime import datetime
    ```

    Why: This provides a high-level method that SupabaseStore users can call to ingest embeddings for thousands of sections efficiently. It handles the mapping between section IDs and embeddings, stores results in Supabase, and provides clear success/failure reporting.

    Integration: Uses batch_generate_embeddings() from Task 1, maintaining separation of concerns (EmbeddingService handles API calls, SupabaseStore handles storage).
  </action>
  <verify>
    1. Check syntax: `python -c "from core.supabase_store import SupabaseStore; print('Import successful')"`
       Expected: No import errors
    2. Verify method exists:
       ```python
       from core.supabase_store import SupabaseStore
       assert hasattr(SupabaseStore, 'batch_ingest_embeddings')
       ```
       Expected: Method exists
    3. Check return type (mock test):
       ```python
       # Verify method returns dict with correct keys
       # Should return: success_count, failed_count, total_tokens, failed_ids
       ```
       Expected: Returns dict with 'success_count', 'failed_count', 'total_tokens', 'failed_ids' keys
  </verify>
  <done>SupabaseStore has batch_ingest_embeddings() method that uses batch_generate_embeddings() for efficient large-scale ingestion.</done>
</task>

<task type="auto">
  <name>Task 3: Update existing batch_generate() comment and add deprecation notice</name>
  <files>core/embedding_service.py</files>
  <action>
    Update the existing batch_generate() method to add deprecation notice and clarify it's superseded by batch_generate_embeddings():

    Update method docstring (lines 100-115):
    ```python
    def batch_generate(self, texts: List[str], max_batch_size: int = 100) -> List[List[float]]:
        """
        Generate embeddings in batch for efficiency.

        .. deprecated::
            Use batch_generate_embeddings() instead for better performance.
            This method is limited to 100 embeddings per call, while
            batch_generate_embeddings() supports up to 2048.

        Legacy method for batch embedding generation. OpenAI API supports
        up to 2048 texts per request, but this method uses conservative
        100-item limit for backward compatibility.

        Args:
            texts: List of text strings to embed
            max_batch_size: Max texts per API call (default: 100, legacy limit)

        Returns:
            List of embedding vectors, same length as input

        Raises:
            ValueError: If texts list is empty or contains empty strings
            RuntimeError: If OpenAI API call fails
        """
    ```

    Also update class-level comment on line 101 to clarify the limit difference:
    ```python
    # Legacy batch limit (new batch_generate_embeddings supports 2048)
    ```

    Why: Maintains backward compatibility while directing users to the more efficient new method. Clear deprecation notice prevents confusion about which method to use for new code.

    Note: Do NOT remove the existing method as it may be used by existing code. Just add deprecation notice.
  </action>
  <verify>
    1. Check docstring updated: `python -c "from core.embedding_service import EmbeddingService; print(EmbeddingService.batch_generate.__doc__[:100])"`
       Expected: Docstring contains "deprecated" or "Use batch_generate_embeddings"
    2. Verify backward compatibility:
       ```python
       from core.embedding_service import EmbeddingService
       # Old method should still work
       assert hasattr(EmbeddingService, 'batch_generate')
       ```
       Expected: Method still exists and callable
  </verify>
  <done>Existing batch_generate() has deprecation notice directing users to batch_generate_embeddings(). Backward compatibility maintained.</done>
</task>

<task type="verification">
  <name>Task 4: Verify all existing embedding tests pass</name>
  <files>test/test_embedding_service.py</files>
  <action>
    Run all existing embedding service tests to ensure no regressions:

    1. Run embedding service tests:
       ```bash
       python -m pytest test/test_embedding_service.py -v
       ```

    2. Verify no test failures or errors

    3. Check that existing functionality is not broken:
       - generate_embedding() still works
       - batch_generate() still works (backward compatibility)
       - get_or_generate_embedding() still works
       - Caching still works
       - Token tracking still works

    Why: Success criterion requires "all existing embedding tests pass with batch implementation". This ensures the new batch functionality doesn't break existing code.
  </action>
  <verify>
    Run: `python -m pytest test/test_embedding_service.py -v --tb=short`
    Expected: All tests pass (no failures, no errors)
    Pass criteria: 100% of existing tests pass
  </verify>
  <done>All existing embedding service tests pass with new batch implementation. No regressions detected.</done>
</task>

</tasks>

<verification>
Overall verification after all tasks complete:
1. Run all embedding tests: `python -m pytest test/test_embedding_service.py -v`
   Expected: All tests pass
2. Test batch method with small dataset (manual):
   ```python
   from core.embedding_service import EmbeddingService
   from unittest.mock import Mock, patch

   with patch('core.embedding_service.OpenAI'):
       service = EmbeddingService(api_key="test")
       result = service.batch_generate_embeddings(["test1", "test2", "test3"])
       assert 'embeddings' in result
       assert 'failed_indices' in result
       assert 'total_tokens' in result
   ```
   Expected: Returns dict with correct structure
3. Verify batch size constant:
   ```python
   from core.embedding_service import EmbeddingService
   assert EmbeddingService.MAX_BATCH_SIZE == 2048
   ```
   Expected: MAX_BATCH_SIZE = 2048
4. Check SupabaseStore method exists:
   ```python
   from core.supabase_store import SupabaseStore
   assert hasattr(SupabaseStore, 'batch_ingest_embeddings')
   ```
   Expected: Method exists
5. Verify backward compatibility:
   ```python
   from core.embedding_service import EmbeddingService
   assert hasattr(EmbeddingService, 'batch_generate')
   ```
   Expected: Legacy method still exists
</verification>

<success_criteria>
1. EmbeddingService.batch_generate_embeddings() method implemented with 2048 embeddings per batch
2. Method returns dict with embeddings, failed_indices, and total_tokens
3. Graceful failure handling: continues processing on batch errors
4. SupabaseStore.batch_ingest_embeddings() method implemented using batch API
5. Existing batch_generate() method has deprecation notice
6. All existing embedding tests pass without modification
7. Backward compatibility maintained for existing code
8. MAX_BATCH_SIZE constant set to 2048 (OpenAI's actual limit)
</success_criteria>

<output>
After completion, create `.planning/phases/02-batch_embeddings/02-01-SUMMARY.md` with:
- New methods added (batch_generate_embeddings, batch_ingest_embeddings)
- Performance improvement: 2048 embeddings per batch vs 100 (20x improvement)
- Graceful failure handling implementation details
- Verification results (existing tests passing)
- Any edge cases discovered and handled
- Backward compatibility notes
</output>
