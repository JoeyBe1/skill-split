---
phase: 01-hybrid_search_scoring
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - core/database.py
  - core/query.py
  - core/hybrid_search.py
autonomous: true

must_haves:
  truths:
    - "User searches for 'vector search' and finds sections about vector search regardless of position in file"
    - "User searches for 'python handler' and sees relevant Python sections ranked higher than unrelated sections"
    - "Hybrid search results include normalized relevance scores combining text and vector similarity"
    - "All existing tests pass with new scoring implementation"
  artifacts:
    - path: "core/database.py"
      provides: "FTS5 virtual table and text search with bm25 ranking"
      contains: "CREATE VIRTUAL TABLE sections_fts USING fts5"
      min_lines: 120
    - path: "core/query.py"
      provides: "Text search returning (section_id, rank) tuples with relevance scores"
      exports: ["search_sections_with_rank"]
      min_lines: 40
    - path: "core/hybrid_search.py"
      provides: "Updated text_search using FTS5 rank scores instead of position"
      contains: "bm25 ranking"
      min_lines: 50
  key_links:
    - from: "core/hybrid_search.py:text_search()"
      to: "core/query.py:search_sections_with_rank()"
      via: "QueryAPI method call returning (section_id, rank) tuples"
      pattern: "search_sections_with_rank"
    - from: "core/query.py:search_sections_with_rank()"
      to: "sections_fts virtual table"
      via: "FTS5 bm25() function for relevance ranking"
      pattern: "bm25\\(sections_fts\\)"
    - from: "core/database.py:_create_schema()"
      to: "sections_fts virtual table"
      via: "FTS5 table creation with content=sections linkage"
      pattern: "CREATE VIRTUAL TABLE.*fts5"
---

<objective>
Implement FTS5-based text scoring with proper relevance ranking to replace placeholder position-based scoring in hybrid search.

Purpose: Current text search in hybrid_search.py uses position-based scoring (first result = highest score) which doesn't reflect actual text relevance. FTS5 with BM25 ranking provides proper term frequency and inverse document frequency scoring.

Output: FTS5 virtual table, updated QueryAPI search method returning ranked results, and hybrid_search.py using actual text relevance scores.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@core/database.py
@core/query.py
@core/hybrid_search.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add FTS5 virtual table to DatabaseStore</name>
  <files>core/database.py</files>
  <action>
    Add FTS5 full-text search virtual table to database schema:

    1. In _create_schema() method, after creating the sections table, add:
       ```python
       # Create FTS5 virtual table for full-text search
       conn.execute(
           """
           CREATE VIRTUAL TABLE IF NOT EXISTS sections_fts USING fts5(
               title,
               content,
               content=sections,
               content_rowid=id
           )
           """
       )

       # Populate FTS table with existing data (only on new creation)
       conn.execute(
           """
           INSERT INTO sections_fts(rowid, title, content)
           SELECT id, title, content FROM sections
           WHERE NOT EXISTS (SELECT 1 FROM sections_fts WHERE sections_fts.rowid = sections.id)
           """
       )
       ```

    2. In store_file() method, after storing sections, add FTS synchronization:
       ```python
       # After _store_sections() call, add:
       # Sync FTS table
       for section in doc.sections:
           self._sync_section_fts(conn, file_id, section, parent_id=None)
       ```

    3. Add helper method _sync_section_fts():
       ```python
       def _sync_section_fts(self, conn, file_id, section, parent_id=None):
           """Sync a single section to FTS table."""
           # Get section_id from last insert or query
           cursor = conn.execute(
               "SELECT id FROM sections WHERE file_id = ? AND title = ? AND line_start = ?",
               (file_id, section.title, section.line_start)
           )
           row = cursor.fetchone()
           if row:
               section_id = row[0]
               conn.execute(
                   "INSERT OR REPLACE INTO sections_fts(rowid, title, content) VALUES (?, ?, ?)",
                   (section_id, section.title, section.content)
               )
           # Recursively sync children
           if section.children:
               for child in section.children:
                   self._sync_section_fts(conn, file_id, child, section_id)
       ```

    Why: FTS5 provides BM25 ranking algorithm for proper text relevance scoring. The external content table approach keeps FTS index synchronized with main sections table automatically.
  </action>
  <verify>
    1. Run: `sqlite3 skill_split.db ".schema sections_fts"`
       Expected: Output shows CREATE VIRTUAL TABLE statement
    2. Run: `python -c "from core.database import DatabaseStore; db = DatabaseStore('skill_split.db'); print('FTS table created')"`
       Expected: No errors, FTS table exists
  </verify>
  <done>FTS5 virtual table exists and is synchronized with sections table. BM25 ranking available via bm25() function.</done>
</task>

<task type="auto">
  <name>Task 2: Add ranked text search method to QueryAPI</name>
  <files>core/query.py</files>
  <action>
    Add new method search_sections_with_rank() that returns (section_id, rank) tuples with FTS5 relevance scores:

    Add after search_sections() method:
    ```python
    def search_sections_with_rank(
        self, query: str, file_path: Optional[str] = None
    ) -> List[tuple[int, float]]:
        """
        Search sections using FTS5 full-text search with relevance ranking.

        Uses BM25 algorithm for ranking based on term frequency and
        inverse document frequency. Returns results with relevance scores
        where higher = more relevant.

        Args:
            query: Search string
            file_path: Optional file path to limit search to one file

        Returns:
            List of (section_id, rank) tuples where rank is negative BM25 score
            (higher values = more relevant, so we negate for compatibility)
        """
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row

            if file_path:
                # Single file search with FTS
                cursor = conn.execute(
                    """
                    SELECT s.id, bm25(sections_fts) as rank
                    FROM sections_fts
                    JOIN sections s ON sections_fts.rowid = s.id
                    JOIN files f ON s.file_id = f.id
                    WHERE sections_fts MATCH ? AND f.path = ?
                    ORDER BY rank
                    """,
                    (query, file_path)
                )
            else:
                # Cross-file search with FTS
                cursor = conn.execute(
                    """
                    SELECT s.id, bm25(sections_fts) as rank
                    FROM sections_fts
                    JOIN sections s ON sections_fts.rowid = s.id
                    WHERE sections_fts MATCH ?
                    ORDER BY rank
                    """,
                    (query,)
                )

            # Return (section_id, normalized_rank) tuples
            # BM25 returns negative scores, negate for "higher = better"
            results = [(row["id"], -row["rank"]) for row in cursor.fetchall()]
            return results
    ```

    Why: Current search_sections() returns Section objects without relevance scores. New method returns (id, rank) tuples that hybrid_search can use directly. BM25 provides proper ranking based on term frequency and document frequency.

    Note: FTS5 MATCH syntax uses natural query syntax (spaces = OR, quotes = phrases, AND = implicit for all terms).
  </action>
  <verify>
    1. Run: `python -c "from core.query import QueryAPI; q = QueryAPI('skill_split.db'); r = q.search_sections_with_rank('python'); print(f'Found {len(r)} results'); print(f'First result: id={r[0][0]}, rank={r[0][1]:.2f}' if r else 'No results')"`
       Expected: Returns list of tuples with numeric ranks (not None)
    2. Add debug: Confirm rank values differ between results (not position-based)
  </verify>
  <done>QueryAPI has search_sections_with_rank() returning (section_id, rank) tuples with BM25 relevance scores.</done>
</task>

<task type="auto">
  <name>Task 3: Update hybrid_search.py to use FTS5 ranking</name>
  <files>core/hybrid_search.py</files>
  <action>
    Replace placeholder text_search() scoring with actual FTS5 relevance scores:

    1. Update text_search() method (lines 161-196):
       ```python
       def text_search(
           self,
           query: str,
           limit: int = 10
       ) -> List[Tuple[int, float]]:
           """
           Search sections using FTS5 full-text search with BM25 ranking.

           Uses QueryAPI search_sections_with_rank which performs
           proper relevance-based ranking using FTS5's BM25 algorithm.

           Args:
               query: Search query string
               limit: Maximum results to return

           Returns:
               List of (section_id, relevance_score) tuples where higher = more relevant
           """
           try:
               # Use QueryAPI's FTS5 ranked search
               results = self.query_api.search_sections_with_rank(query)

               # Normalize scores to [0, 1] for hybrid combination
               if not results:
                   return []

               # Extract ranks (already negated in QueryAPI so higher = better)
               section_ids, ranks = zip(*results)
               min_rank = min(ranks)
               max_rank = max(ranks)

               # Normalize to [0, 1]
               if max_rank == min_rank:
                   normalized_scores = [0.5] * len(ranks)
               else:
                   normalized_scores = [
                       (r - min_rank) / (max_rank - min_rank)
                       for r in ranks
                   ]

               # Return top N results with normalized scores
               scored_results = list(zip(section_ids, normalized_scores))[:limit]

               self.metrics["text_searches"] += 1
               return scored_results

           except Exception as e:
               raise RuntimeError(f"Text search failed: {str(e)}")
       ```

    2. Remove comment about position-based scoring at line 184-188

    Why: FTS5 BM25 provides proper relevance ranking based on term frequency, inverse document frequency, and document length normalization. Score normalization ensures fair combination with vector similarity in hybrid_score().

    Key changes:
    - Calls search_sections_with_rank() instead of search_sections()
    - Uses BM25 relevance scores instead of position
    - Normalizes scores to [0, 1] for hybrid combination
  </action>
  <verify>
    1. Run: `python -m pytest test/test_hybrid_search.py::TestTextSearch -v`
       Expected: All tests pass (may need minor updates for rank-based scoring)
    2. Run integration test:
       ```python
       from core.hybrid_search import HybridSearch
       from unittest.mock import Mock
       embedding_service = Mock()
       embedding_service.generate_embedding.return_value = [0.1, 0.2, 0.3]
       query_api = Mock()
       # Mock FTS search returning ranked results
       query_api.search_sections_with_rank.return_value = [(1, 2.5), (2, 1.8), (3, 0.9)]
       supabase_store = Mock()
       hybrid = HybridSearch(embedding_service, supabase_store, query_api)
       results = hybrid.text_search("python handler", limit=10)
       print(f"Results: {results}")
       # Expected: [(1, 1.0), (2, ~0.6), (3, 0.0)] (normalized scores)
       ```
    3. Run: `python -m pytest test/test_hybrid_search.py -v`
       Expected: All existing tests pass with new scoring
  </verify>
  <done>text_search() uses FTS5 BM25 relevance scores normalized to [0, 1]. Hybrid search combines proper text relevance with vector similarity.</done>
</task>

</tasks>

<verification>
Overall verification after all tasks complete:
1. Run all tests: `python -m pytest test/test_hybrid_search.py -v`
   Expected: All tests pass
2. Run integration tests: `python -m pytest test/test_hybrid_search_integration.py -v`
   Expected: All integration tests pass
3. Test actual search behavior:
   ```python
   from core.query import QueryAPI
   q = QueryAPI('skill_split.db')
   results = q.search_sections_with_rank('python')
   # Should return ranked results by relevance, not position
   assert len(results) > 0, "FTS search returns results"
   assert all(isinstance(r, tuple) and len(r) == 2 for r in results), "Results are (id, rank) tuples"
   ```
4. Verify FTS5 table: `sqlite3 skill_split.db "SELECT count(*) FROM sections_fts;"`
   Expected: Row count matches sections table
</verification>

<success_criteria>
1. FTS5 virtual table created and synchronized with sections table
2. QueryAPI.search_sections_with_rank() returns (section_id, rank) tuples
3. hybrid_search.py text_search() uses normalized BM25 scores
4. All existing tests pass without modification
5. Search results ranked by text relevance, not position
</success_criteria>

<output>
After completion, create `.planning/phases/01-hybrid_search_scoring/01-01-SUMMARY.md` with:
- Changes made to each file
- Test results
- Example showing improved ranking (before/after comparison)
- Any issues encountered and resolutions
</output>
