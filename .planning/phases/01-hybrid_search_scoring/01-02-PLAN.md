---
phase: 01-hybrid_search_scoring
plan: 02
type: tdd
wave: 2
depends_on: ["01-01"]
files_modified:
  - test/test_hybrid_search.py
  - test/test_query.py
autonomous: true

must_haves:
  truths:
    - "User searches for 'vector search' and finds sections about vector search regardless of position in file"
    - "User searches for 'python handler' and sees relevant Python sections ranked higher than unrelated sections"
    - "Hybrid search results include normalized relevance scores combining text and vector similarity"
    - "All existing tests pass with new scoring implementation"
  artifacts:
    - path: "test/test_hybrid_search.py"
      provides: "Tests for FTS5-based text search relevance ranking"
      contains: "test_text_search_uses_fts5_ranking"
      min_lines: 50
    - path: "test/test_query.py"
      provides: "Tests for QueryAPI.search_sections_with_rank()"
      contains: "test_search_sections_with_rank_returns_relevance_scores"
      min_lines: 30
  key_links:
    - from: "test/test_query.py::TestQueryAPI::test_search_sections_with_rank"
      to: "core/query.py:search_sections_with_rank()"
      via: "Unit test verifying (section_id, rank) tuple format and BM25 ranking"
      pattern: "assert.*rank"
    - from: "test/test_hybrid_search.py::TestTextSearch::test_text_search_uses_fts5_ranking"
      to: "core/hybrid_search.py:text_search()"
      via: "Integration test verifying FTS5 rank usage and normalization"
      pattern: "search_sections_with_rank"
    - from: "test/test_hybrid_search.py::TestTextSearchQuality"
      to: "production search behavior"
      via: "Quality tests confirming relevance-based ranking"
      pattern: "relevance|higher.*rank"
---

<objective>
Add text search quality tests for relevance verification, ensuring FTS5 BM25 ranking provides better results than position-based scoring.

Purpose: Verify that FTS5 implementation actually improves search quality by testing relevance ranking with real queries. Tests should confirm that semantically relevant sections rank higher regardless of position.

Output: Comprehensive test suite for FTS5 text search with relevance quality verification.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@test/test_hybrid_search.py
@test/test_query.py
@core/query.py
@core/hybrid_search.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add FTS5 ranking tests to test_query.py</name>
  <files>test/test_query.py</files>
  <action>
    Add tests for QueryAPI.search_sections_with_rank() method.

    Add new test class:
    ```python
    class TestQueryAPIFTS5Ranking:
        """Test FTS5 full-text search with BM25 ranking."""

        def test_search_sections_with_rank_returns_tuples(self, db_store):
            """search_sections_with_rank() returns (section_id, rank) tuples."""
            from core.query import QueryAPI

            # Store test data
            # ... (existing test data setup)

            query_api = QueryAPI(db_store.db_path)
            results = query_api.search_sections_with_rank("test")

            assert all(isinstance(r, tuple) and len(r) == 2 for r in results)
            assert all(isinstance(r[0], int) for r in results)  # section_id
            assert all(isinstance(r[1], float) for r in results)  # rank

        def test_search_sections_with_rank_ranking_quality(self, db_store):
            """BM25 ranking prioritizes relevant matches over position."""
            from core.query import QueryAPI

            # Create test sections where relevant content is NOT first
            # ... setup sections with "python" in different positions

            query_api = QueryAPI(db_store.db_path)
            results = query_api.search_sections_with_rank("python")

            # Results should be ranked by relevance, not position
            # Extract section contents and verify ranking
            assert len(results) > 1
            # Most relevant result should have highest rank
            ranks = [r[1] for r in results]
            assert ranks == sorted(ranks, reverse=True), "Ranks should be descending"

        def test_search_sections_with_rank_case_insensitive(self, db_store):
            """FTS5 search is case-insensitive."""
            from core.query import QueryAPI

            query_api = QueryAPI(db_store.db_path)

            # Search with different cases
            results_lower = query_api.search_sections_with_rank("python")
            results_upper = query_api.search_sections_with_rank("PYTHON")
            results_mixed = query_api.search_sections_with_rank("Python")

            # Should return same results
            assert len(results_lower) == len(results_upper) == len(results_mixed)

        def test_search_sections_with_rank_empty_query(self, db_store):
            """Empty query returns empty results."""
            from core.query import QueryAPI

            query_api = QueryAPI(db_store.db_path)
            results = query_api.search_sections_with_rank("")

            # FTS5 MATCH with empty string returns no results
            assert results == []

        def test_search_sections_with_rank_file_filter(self, db_store):
            """file_path parameter restricts search to specific file."""
            from core.query import QueryAPI

            query_api = QueryAPI(db_store.db_path)

            all_results = query_api.search_sections_with_rank("test")
            file_results = query_api.search_sections_with_rank("test", file_path="/test/path.md")

            # File-restricted results should be subset of all results
            assert len(file_results) <= len(all_results)
    ```

    Why: Tests verify that search_sections_with_rank() returns proper format and uses BM25 ranking. Quality test confirms ranking is relevance-based, not position-based.
  </action>
  <verify>
    Run: `python -m pytest test/test_query.py::TestQueryAPIFTS5Ranking -v`
    Expected: All 5 tests pass
  </verify>
  <done>test_query.py has comprehensive FTS5 ranking tests verifying tuple format, BM25 ranking quality, and filtering.</done>
</task>

<task type="auto">
  <name>Task 2: Add relevance quality tests to test_hybrid_search.py</name>
  <files>test/test_hybrid_search.py</files>
  <action>
    Add tests for text search relevance quality in hybrid search context.

    Add new test class:
    ```python
    class TestTextSearchQuality:
        """Test text search relevance quality with FTS5."""

        @pytest.fixture
        def quality_hybrid_search(self):
            """Create HybridSearch with test data."""
            from core.hybrid_search import HybridSearch
            from unittest.mock import Mock
            import tempfile
            import os

            # Create temporary database with test sections
            from core.database import DatabaseStore
            from core.parser import Parser
            from models import ParsedDocument, Section, FileType

            db_fd, db_path = tempfile.mkstemp(suffix='.db')
            os.close(db_fd)

            store = DatabaseStore(db_path)

            # Create test document with "vector search" content in different sections
            sections = [
                Section(level=1, title="Introduction", content="Basic intro text", line_start=1, line_end=2),
                Section(level=1, title="Vector Search", content="This section discusses vector search implementation using embeddings and similarity metrics.", line_start=3, line_end=4),
                Section(level=1, title="Database", content="Database storage and retrieval", line_start=5, line_end=6),
                Section(level=2, title="Python Handler", content="Python handler processes files and creates embeddings for vector search.", line_start=7, line_end=8),
            ]

            doc = ParsedDocument(
                file_type=FileType.MARKDOWN,
                frontmatter="",
                sections=sections
            )
            store.store_file("/test/search.md", doc, "test_hash")

            # Create dependencies
            from core.query import QueryAPI
            query_api = QueryAPI(db_path)

            embedding_service = Mock()
            embedding_service.generate_embedding.return_value = [0.1, 0.2, 0.3]

            supabase_store = Mock()

            hybrid = HybridSearch(embedding_service, supabase_store, query_api)

            yield hybrid, db_path

            # Cleanup
            os.unlink(db_path)

        def test_text_search_finds_relevant_content(self, quality_hybrid_search):
            """Text search finds content regardless of position."""
            hybrid, _ = quality_hybrid_search

            results = hybrid.text_search("vector search", limit=10)

            # Should find sections about vector search
            assert len(results) > 0

            # Get section IDs
            section_ids = [r[0] for r in results]
            # Sections 2 and 4 contain "vector search"
            # They should be in results regardless of position
            assert len(section_ids) >= 1

        def test_text_search_ranks_relevant_higher(self, quality_hybrid_search):
            """Relevant sections get higher scores than irrelevant sections."""
            hybrid, _ = quality_hybrid_search

            results = hybrid.text_search("vector search", limit=10)

            if len(results) >= 2:
                # First result should have higher score than last
                assert results[0][1] >= results[-1][1]

        def test_text_search_normalizes_scores(self, quality_hybrid_search):
            """Text search scores are normalized to [0, 1]."""
            hybrid, _ = quality_hybrid_search

            results = hybrid.text_search("python handler", limit=10)

            if results:
                # All scores should be in [0, 1]
                scores = [r[1] for r in results]
                assert all(0.0 <= s <= 1.0 for s in scores)

        def test_text_search_vs_position_ranking(self, quality_hybrid_search):
            """FTS5 ranking differs from simple position-based ranking."""
            hybrid, _ = quality_hybrid_search

            results_fts = hybrid.text_search("search", limit=10)

            # FTS results should use actual relevance
            # If we had position-based scoring, first result might differ
            assert len(results_fts) > 0

            # Verify scores vary (not all the same)
            if len(results_fts) > 1:
                scores = [r[1] for r in results_fts]
                # Scores should not all be identical
                assert len(set(scores)) > 1, "Scores should vary based on relevance"

        def test_hybrid_search_combines_text_and_vector(self, quality_hybrid_search):
            """Hybrid search combines FTS text scores with vector similarity."""
            hybrid, db_path = quality_hybrid_search

            # Mock vector search
            hybrid.supabase_store.client.rpc.return_value.execute.return_value.data = [
                {'section_id': 1, 'similarity': 0.8},
                {'section_id': 2, 'similarity': 0.9},
            ]

            results = hybrid.hybrid_search("vector search", limit=5, vector_weight=0.7)

            # Results should combine both sources
            assert len(results) > 0

            # Scores should be normalized
            scores = [r[1] for r in results]
            assert all(0.0 <= s <= 1.0 for s in scores)
    ```

    Why: Quality tests verify that FTS5 actually improves search relevance. Tests use real content to confirm that semantically relevant sections rank higher, not just earlier sections.
  </action>
  <verify>
    Run: `python -m pytest test/test_hybrid_search.py::TestTextSearchQuality -v`
    Expected: All 5 tests pass
  </verify>
  <done>test_hybrid_search.py has quality tests confirming FTS5 relevance ranking improves search results.</done>
</task>

</tasks>

<verification>
Overall verification:
1. Run all new tests: `python -m pytest test/test_query.py::TestQueryAPIFTS5Ranking test/test_hybrid_search.py::TestTextSearchQuality -v`
   Expected: All 10 tests pass
2. Run full test suite: `python -m pytest test/test_hybrid_search.py test/test_query.py -v`
   Expected: All tests pass (existing + new)
3. Verify test coverage: `python -m pytest test/test_hybrid_search.py test/test_query.py --cov=core/hybrid_search --cov=core/query --cov-report=term-missing`
   Expected: >90% coverage for new methods
4. Manual verification:
   ```python
   from core.query import QueryAPI
   q = QueryAPI('skill_split.db')
   results = q.search_sections_with_rank('vector search')
   # Show that ranking is relevance-based
   for section_id, rank in results[:5]:
       section = q.get_section(section_id)
       print(f"[{rank:.2f}] {section.title}: {section.content[:50]}...")
   ```
   Expected: Higher ranks for sections actually about vector search
</verification>

<success_criteria>
1. All new tests pass
2. All existing tests continue to pass
3. Tests verify (section_id, rank) tuple format
4. Tests confirm BM25 ranking improves over position-based
5. Quality tests demonstrate relevance-based ordering
</success_criteria>

<output>
After completion, create `.planning/phases/01-hybrid_search_scoring/01-02-SUMMARY.md` with:
- Tests added to each test file
- Test coverage metrics
- Example test output showing improved ranking
- Any edge cases discovered and handled
</output>
