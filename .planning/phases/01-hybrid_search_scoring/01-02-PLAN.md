---
phase: 01-hybrid_search_scoring
plan: 02
type: tdd
wave: 2
depends_on: ["01-01"]
files_modified:
  - test/test_hybrid_search.py
  - test/test_query.py
  - test/test_database.py
autonomous: true

must_haves:
  truths:
    - "User searches for 'vector search' and finds sections about vector search regardless of position in file"
    - "User searches for 'python handler' and sees relevant Python sections ranked higher than unrelated sections"
    - "Hybrid search results include normalized relevance scores combining text and vector similarity"
    - "All existing tests pass with new scoring implementation"
  artifacts:
    - path: "test/test_database.py"
      provides: "Tests for DatabaseStore.search_sections_with_rank() FTS5 BM25 ranking"
      contains: "test_search_sections_with_rank"
      min_lines: 60
    - path: "test/test_query.py"
      provides: "Tests for QueryAPI.search_sections_with_rank() delegation to DatabaseStore"
      contains: "test_search_sections_with_rank_delegates"
      min_lines: 40
    - path: "test/test_hybrid_search.py"
      provides: "Tests for FTS5-based text search relevance ranking"
      contains: "test_text_search_uses_fts5_ranking"
      min_lines: 50
  key_links:
    - from: "test/test_database.py::TestDatabaseStoreFTS5::test_search_sections_with_rank"
      to: "core/database.py:search_sections_with_rank()"
      via: "Unit test verifying (section_id, rank) tuple format and BM25 ranking"
      pattern: "assert.*rank"
    - from: "test/test_query.py::TestQueryAPIFTS5::test_search_sections_with_rank_delegates"
      to: "core/query.py:search_sections_with_rank()"
      via: "Unit test verifying delegation to DatabaseStore"
      pattern: "store\.search_sections_with_rank|mock_store"
    - from: "test/test_hybrid_search.py::TestTextSearch::test_text_search_uses_fts5_ranking"
      to: "core/hybrid_search.py:text_search()"
      via: "Integration test verifying FTS5 rank usage and normalization"
      pattern: "search_sections_with_rank"
    - from: "test/test_hybrid_search.py::TestTextSearchQuality"
      to: "production search behavior"
      via: "Quality tests confirming relevance-based ranking"
      pattern: "relevance|higher.*rank"
---

<objective>
Add text search quality tests for relevance verification, ensuring FTS5 BM25 ranking provides better results than position-based scoring.

Purpose: Verify that FTS5 implementation actually improves search quality by testing relevance ranking with real queries. Tests should confirm that semantically relevant sections rank higher regardless of position, and that the architectural delegation pattern is correctly implemented.

Output: Comprehensive test suite for FTS5 text search with relevance quality verification and architectural pattern validation.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/codebase/ARCHITECTURE.md

@test/test_hybrid_search.py
@test/test_query.py
@test/test_database.py
@core/query.py
@core/hybrid_search.py
@core/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add FTS5 ranking tests to test_database.py</name>
  <files>test/test_database.py</files>
  <action>
    Add tests for DatabaseStore.search_sections_with_rank() method.

    Add new test class:
    ```python
    class TestDatabaseStoreFTS5:
        """Test FTS5 full-text search with BM25 ranking in DatabaseStore."""

        def test_search_sections_with_rank_returns_tuples(self, db_store):
            """search_sections_with_rank() returns (section_id, rank) tuples."""
            # Store test data using existing fixtures
            from models import ParsedDocument, Section, FileType
            doc = ParsedDocument(
                file_type=FileType.MARKDOWN,
                frontmatter="",
                sections=[
                    Section(level=1, title="Python", content="Python code here", line_start=1, line_end=2),
                    Section(level=1, title="JavaScript", content="JS code here", line_start=3, line_end=4),
                ]
            )
            db_store.store_file("/test/search.md", doc, "test_hash")

            results = db_store.search_sections_with_rank("python")

            assert all(isinstance(r, tuple) and len(r) == 2 for r in results)
            assert all(isinstance(r[0], int) for r in results)  # section_id
            assert all(isinstance(r[1], float) for r in results)  # rank

        def test_search_sections_with_rank_ranking_quality(self, db_store):
            """BM25 ranking prioritizes relevant matches over position."""
            from models import ParsedDocument, Section, FileType

            # Create test sections where relevant content is NOT first
            sections = [
                Section(level=1, title="Intro", content="Basic intro text", line_start=1, line_end=2),
                Section(level=1, title="Python", content="Some python code", line_start=3, line_end=4),
                Section(level=2, title="Python Advanced", content="Advanced python handler with detailed implementation", line_start=5, line_end=6),
                Section(level=1, title="Database", content="Database storage", line_start=7, line_end=8),
            ]

            doc = ParsedDocument(
                file_type=FileType.MARKDOWN,
                frontmatter="",
                sections=sections
            )
            db_store.store_file("/test/python.md", doc, "test_hash")

            results = db_store.search_sections_with_rank("python handler")

            # Results should be ranked by relevance, not position
            # "Python Advanced" should rank higher than "Python" due to "handler" match
            assert len(results) > 1
            ranks = [r[1] for r in results]
            assert ranks == sorted(ranks, reverse=True), "Ranks should be descending"

        def test_search_sections_with_rank_case_insensitive(self, db_store):
            """FTS5 search is case-insensitive."""
            from models import ParsedDocument, Section, FileType

            doc = ParsedDocument(
                file_type=FileType.MARKDOWN,
                frontmatter="",
                sections=[Section(level=1, title="Python", content="Python content", line_start=1, line_end=2)]
            )
            db_store.store_file("/test/case.md", doc, "test_hash")

            results_lower = db_store.search_sections_with_rank("python")
            results_upper = db_store.search_sections_with_rank("PYTHON")
            results_mixed = db_store.search_sections_with_rank("Python")

            # Should return same results
            assert len(results_lower) == len(results_upper) == len(results_mixed)

        def test_search_sections_with_rank_empty_query(self, db_store):
            """Empty query returns empty results."""
            results = db_store.search_sections_with_rank("")
            # FTS5 MATCH with empty string returns no results
            assert results == []

        def test_search_sections_with_rank_file_filter(self, db_store):
            """file_path parameter restricts search to specific file."""
            from models import ParsedDocument, Section, FileType

            doc1 = ParsedDocument(
                file_type=FileType.MARKDOWN,
                frontmatter="",
                sections=[Section(level=1, title="Test", content="Test content", line_start=1, line_end=2)]
            )
            db_store.store_file("/test/path1.md", doc1, "hash1")

            doc2 = ParsedDocument(
                file_type=FileType.MARKDOWN,
                frontmatter="",
                sections=[Section(level=1, title="Test", content="Test content", line_start=1, line_end=2)]
            )
            db_store.store_file("/test/path2.md", doc2, "hash2")

            all_results = db_store.search_sections_with_rank("test")
            file_results = db_store.search_sections_with_rank("test", file_path="/test/path1.md")

            # File-restricted results should be subset of all results
            assert len(file_results) <= len(all_results)
    ```

    Why: Tests verify that search_sections_with_rank() in DatabaseStore returns proper format and uses BM25 ranking. Quality test confirms ranking is relevance-based, not position-based. This tests the actual database layer implementation.
  </action>
  <verify>
    Run: `python -m pytest test/test_database.py::TestDatabaseStoreFTS5 -v`
    Expected: All 5 tests pass
  </verify>
  <done>test_database.py has comprehensive FTS5 ranking tests verifying tuple format, BM25 ranking quality, and filtering.</done>
</task>

<task type="auto">
  <name>Task 2: Add delegation tests to test_query.py</name>
  <files>test/test_query.py</files>
  <action>
    Add tests for QueryAPI.search_sections_with_rank() method, focusing on delegation pattern.

    Add new test class:
    ```python
    class TestQueryAPIFTS5:
        """Test QueryAPI FTS5 search delegation to DatabaseStore."""

        def test_search_sections_with_rank_delegates_to_store(self, query_api):
            """search_sections_with_rank() delegates to DatabaseStore."""
            from unittest.mock import patch

            with patch.object(query_api.store, 'search_sections_with_rank') as mock_search:
                mock_search.return_value = [(1, 2.5), (2, 1.8)]

                results = query_api.search_sections_with_rank("test query")

                # Verify delegation
                mock_search.assert_called_once_with("test query", None)
                assert results == [(1, 2.5), (2, 1.8)]

        def test_search_sections_with_rank_passes_file_path(self, query_api):
            """file_path parameter is passed through to DatabaseStore."""
            from unittest.mock import patch

            with patch.object(query_api.store, 'search_sections_with_rank') as mock_search:
                mock_search.return_value = [(1, 2.5)]

                results = query_api.search_sections_with_rank("test", file_path="/test/path.md")

                # Verify file_path is passed
                mock_search.assert_called_once_with("test", "/test/path.md")
                assert results == [(1, 2.5)]

        def test_search_sections_with_rank_returns_store_results(self, query_api):
            """Returns results from DatabaseStore unchanged."""
            from unittest.mock import patch

            expected = [(1, 3.5), (2, 2.1), (3, 0.8)]
            with patch.object(query_api.store, 'search_sections_with_rank') as mock_search:
                mock_search.return_value = expected

                results = query_api.search_sections_with_rank("python")

                assert results == expected
                # Verify no transformation of results
                assert len(results) == 3

        def test_search_sections_with_rank_integration(self, query_api, db_store):
            """Integration test with real DatabaseStore."""
            from models import ParsedDocument, Section, FileType

            # Store test data
            doc = ParsedDocument(
                file_type=FileType.MARKDOWN,
                frontmatter="",
                sections=[
                    Section(level=1, title="Python", content="Python handler", line_start=1, line_end=2),
                    Section(level=1, title="Database", content="Database storage", line_start=3, line_end=4),
                ]
            )
            query_api.store.store_file("/test/integration.md", doc, "test_hash")

            results = query_api.search_sections_with_rank("python")

            # Should delegate to store and get results
            assert len(results) > 0
            assert all(isinstance(r, tuple) and len(r) == 2 for r in results)

        def test_search_sections_with_rank_empty_query(self, query_api):
            """Empty query returns empty results via delegation."""
            results = query_api.search_sections_with_rank("")
            assert results == []
    ```

    Why: Tests verify that QueryAPI correctly delegates to DatabaseStore following the architectural pattern. Mock tests verify delegation without requiring real data. Integration test confirms end-to-end behavior matches architectural expectations.
  </action>
  <verify>
    Run: `python -m pytest test/test_query.py::TestQueryAPIFTS5 -v`
    Expected: All 5 tests pass
  </verify>
  <done>test_query.py has delegation tests verifying QueryAPI correctly delegates to DatabaseStore.search_sections_with_rank().</done>
</task>

<task type="auto">
  <name>Task 3: Add relevance quality tests to test_hybrid_search.py</name>
  <files>test/test_hybrid_search.py</files>
  <action>
    Add tests for text search relevance quality in hybrid search context.

    Add new test class:
    ```python
    class TestTextSearchQuality:
        """Test text search relevance quality with FTS5."""

        @pytest.fixture
        def quality_hybrid_search(self):
            """Create HybridSearch with test data."""
            from core.hybrid_search import HybridSearch
            from unittest.mock import Mock
            import tempfile
            import os

            # Create temporary database with test sections
            from core.database import DatabaseStore
            from core.parser import Parser
            from models import ParsedDocument, Section, FileType

            db_fd, db_path = tempfile.mkstemp(suffix='.db')
            os.close(db_fd)

            store = DatabaseStore(db_path)

            # Create test document with "vector search" content in different sections
            sections = [
                Section(level=1, title="Introduction", content="Basic intro text", line_start=1, line_end=2),
                Section(level=1, title="Vector Search", content="This section discusses vector search implementation using embeddings and similarity metrics.", line_start=3, line_end=4),
                Section(level=1, title="Database", content="Database storage and retrieval", line_start=5, line_end=6),
                Section(level=2, title="Python Handler", content="Python handler processes files and creates embeddings for vector search.", line_start=7, line_end=8),
            ]

            doc = ParsedDocument(
                file_type=FileType.MARKDOWN,
                frontmatter="",
                sections=sections
            )
            store.store_file("/test/search.md", doc, "test_hash")

            # Create dependencies
            from core.query import QueryAPI
            query_api = QueryAPI(db_path)

            embedding_service = Mock()
            embedding_service.generate_embedding.return_value = [0.1, 0.2, 0.3]

            supabase_store = Mock()

            hybrid = HybridSearch(embedding_service, supabase_store, query_api)

            yield hybrid, db_path

            # Cleanup
            os.unlink(db_path)

        def test_text_search_finds_relevant_content(self, quality_hybrid_search):
            """Text search finds content regardless of position."""
            hybrid, _ = quality_hybrid_search

            results = hybrid.text_search("vector search", limit=10)

            # Should find sections about vector search
            assert len(results) > 0

            # Get section IDs
            section_ids = [r[0] for r in results]
            # Sections 2 and 4 contain "vector search"
            # They should be in results regardless of position
            assert len(section_ids) >= 1

        def test_text_search_ranks_relevant_higher(self, quality_hybrid_search):
            """Relevant sections get higher scores than irrelevant sections."""
            hybrid, _ = quality_hybrid_search

            results = hybrid.text_search("vector search", limit=10)

            if len(results) >= 2:
                # First result should have higher score than last
                assert results[0][1] >= results[-1][1]

        def test_text_search_normalizes_scores(self, quality_hybrid_search):
            """Text search scores are normalized to [0, 1]."""
            hybrid, _ = quality_hybrid_search

            results = hybrid.text_search("python handler", limit=10)

            if results:
                # All scores should be in [0, 1]
                scores = [r[1] for r in results]
                assert all(0.0 <= s <= 1.0 for s in scores)

        def test_text_search_vs_position_ranking(self, quality_hybrid_search):
            """FTS5 ranking differs from simple position-based ranking."""
            hybrid, _ = quality_hybrid_search

            results_fts = hybrid.text_search("search", limit=10)

            # FTS results should use actual relevance
            # If we had position-based scoring, first result might differ
            assert len(results_fts) > 0

            # Verify scores vary (not all the same)
            if len(results_fts) > 1:
                scores = [r[1] for r in results_fts]
                # Scores should not all be identical
                assert len(set(scores)) > 1, "Scores should vary based on relevance"

        def test_hybrid_search_combines_text_and_vector(self, quality_hybrid_search):
            """Hybrid search combines FTS text scores with vector similarity."""
            hybrid, db_path = quality_hybrid_search

            # Mock vector search
            hybrid.supabase_store.client.rpc.return_value.execute.return_value.data = [
                {'section_id': 1, 'similarity': 0.8},
                {'section_id': 2, 'similarity': 0.9},
            ]

            results = hybrid.hybrid_search("vector search", limit=5, vector_weight=0.7)

            # Results should combine both sources
            assert len(results) > 0

            # Scores should be normalized
            scores = [r[1] for r in results]
            assert all(0.0 <= s <= 1.0 for s in scores)
    ```

    Why: Quality tests verify that FTS5 actually improves search relevance. Tests use real content to confirm that semantically relevant sections rank higher, not just earlier sections. Integration tests confirm the full stack works correctly.
  </action>
  <verify>
    Run: `python -m pytest test/test_hybrid_search.py::TestTextSearchQuality -v`
    Expected: All 5 tests pass
  </verify>
  <done>test_hybrid_search.py has quality tests confirming FTS5 relevance ranking improves search results.</done>
</task>

</tasks>

<verification>
Overall verification:
1. Run all new tests: `python -m pytest test/test_database.py::TestDatabaseStoreFTS5 test/test_query.py::TestQueryAPIFTS5 test/test_hybrid_search.py::TestTextSearchQuality -v`
   Expected: All 15 tests pass
2. Run full test suite: `python -m pytest test/test_hybrid_search.py test/test_query.py test/test_database.py -v`
   Expected: All tests pass (existing + new)
3. Verify test coverage: `python -m pytest test/test_hybrid_search.py test/test_query.py test/test_database.py --cov=core/hybrid_search --cov=core/query --cov=core/database --cov-report=term-missing`
   Expected: >90% coverage for new methods
4. Verify architectural pattern compliance:
   ```python
   # Check that QueryAPI delegates to DatabaseStore
   from core.query import QueryAPI
   from unittest.mock import patch, MagicMock
   q = QueryAPI('test.db')
   with patch.object(q.store, 'search_sections_with_rank') as mock:
       q.search_sections_with_rank('test')
       assert mock.called, "QueryAPI should delegate to DatabaseStore"
   ```
   Expected: Delegation pattern verified
5. Manual verification:
   ```python
   from core.query import QueryAPI
   q = QueryAPI('skill_split.db')
   results = q.search_sections_with_rank('vector search')
   # Show that ranking is relevance-based
   for section_id, rank in results[:5]:
       section = q.get_section(section_id)
       print(f"[{rank:.2f}] {section.title}: {section.content[:50]}...")
   ```
   Expected: Higher ranks for sections actually about vector search
</verification>

<success_criteria>
1. All new tests pass (15 tests across 3 files)
2. All existing tests continue to pass
3. Tests verify (section_id, rank) tuple format
4. Tests confirm BM25 ranking improves over position-based
5. Tests verify QueryAPI delegates to DatabaseStore (architectural compliance)
6. Quality tests demonstrate relevance-based ordering
7. Test coverage >90% for new methods
</success_criteria>

<output>
After completion, create `.planning/phases/01-hybrid_search_scoring/01-02-SUMMARY.md` with:
- Tests added to each test file
- Test coverage metrics
- Example test output showing improved ranking
- Architectural pattern verification results
- Any edge cases discovered and handled
</output>
